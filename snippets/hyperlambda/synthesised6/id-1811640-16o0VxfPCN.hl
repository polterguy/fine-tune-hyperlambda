
// Implement a try/catch logic that limits the number of allowed function invocations. If the number of invocations stored in [.iterations] is less than or equal to 1, log an error, and send multiple [sockets.signal] messages to [.session] (function_error, function_waiting=false, error=true with status 500 and message, and finished=true). If more invocations are allowed, execute the [magic.ai.functions.invoke] slot with arguments [type], [session], [user-id], [invocation] (from [.dp]), and [extra]. Convert the result to JSON with [lambda2json] using pretty formatting. If the token count of the JSON result (using [openai.tokenize]) exceeds [max_context_tokens] from the HTTP endpoint arguments, throw an error. Otherwise, use [config.get] to retrieve a configured success message, unwrap it, and send a [sockets.signal] with the function result, invocation JSON, and workflow file. If the invocation returned a result, append a formatted response with the workflow name and the JSON result to both [.new-prompt] and [.function-result]. If there was no result, append a success message with the workflow name to [.new-prompt].
try
   if
      lte:x:@.iterations
         .:int:1
      .lambda
         log.error:Too many function invocations
         sockets.signal:x:@.session
            args
               function_error:Too many function invocations
         sockets.signal:x:@.session
            args
               function_waiting:bool:false
         sockets.signal:x:@.session
            args
               error:bool:true
               status:int:500
               message:Too many function invocations. Configure your type to handle more invocations or change your prompt.
         sockets.signal:x:@.session
            args
               finished:bool:true
   else
      execute:magic.ai.functions.invoke
         type:x:@.type
         session:x:@.session
         user-id:x:@.user-id
         invocation:x:@.dp/#
         extra:x:@.extra
      lambda2json:x:@execute/*/result/*
         format:true
      if
         mt
            openai.tokenize:x:@lambda2json
            get-value:x:@.arguments/@.arguments/*/max_context_tokens
         .lambda
            throw:Result too large, try to limit your result
               status:int:400
               public:bool:true
      config.get:"magic:chat:functions:success-message"
         .:Success!
      unwrap:x:+/*/*
      sockets.signal:x:@.session
         args
            function_result:x:@config.get
            invocation:x:@execute/*/json
            file:x:@execute/*/workflow
      if
         exists:x:@execute/*/result/*
         .lambda
            set-value:x:@.new-prompt
               strings.concat
                  get-value:x:@.new-prompt
                  .:"Response from '"
                  get-value:x:@execute/*/workflow
                  .:@"' was:
```json
"
                  get-value:x:@lambda2json
                  .:@"
"
                  .:```
                  .:@"
"
                  .:@"
"
            set-value:x:@.function-result
               strings.concat
                  get-value:x:@.function-result
                  get-value:x:@lambda2json
                  .:"\n"
      else
         set-value:x:@.new-prompt
            strings.concat
               get-value:x:@.new-prompt
               .:"Invocation of '"
               get-value:x:@execute/*/workflow
               .:"' was a success."
               .:@"
"
               .:@"
"
