
// Create a conditional check that compares the token count of the [.prompt] argument, as calculated by [openai.tokenize], with the maximum allowed request tokens for the current model (retrieved from [max_request_tokens] under the [.model] node and converted to an integer). If the token count exceeds the limit, throw a public exception with status 400 and a descriptive error message.
if
   mt
      openai.tokenize:x:@.arguments/*/prompt
      convert:x:@.model/*/max_request_tokens
         type:int
   .lambda
      throw:Your request is longer than what this type is configured to allow for
         public:bool:true
         status:int:400
