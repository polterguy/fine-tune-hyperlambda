
// Create a try-catch block that first checks if the number of invocations in [.iterations] is less than or equal to 1 using [lte]. If so, log an error and send multiple [sockets.signal] messages to the session found in [.session], indicating function errors, waiting status, and finalization, including appropriate error details and HTTP status codes. If the condition is not met, execute the [magic.ai.functions.invoke] slot with arguments from [.type], [.session], [.user-id], and the current invocation node. Convert the execution result to JSON using [lambda2json] with formatting enabled. Then check if the token count of the JSON result exceeds the allowed maximum tokens ([max_context_tokens]) by using [openai.tokenize] and [mt]; if it does, throw an error with a 400 status code. Retrieve a success message from configuration using [config.get], unwrap it, and send a [sockets.signal] to the session with the function result, invocation JSON, and workflow file. If there is a result node under the execution, append a formatted response including the workflow name and the JSON result to [.new-prompt], and set [.function-result] to the JSON result. Otherwise, update [.new-prompt] to indicate the invocation was successful, including the workflow name.
try
   if
      lte:x:@.iterations
         .:int:1
      .lambda
         log.error:Too many function invocations
         sockets.signal:x:@.session
            args
               function_error:Too many function invocations
         sockets.signal:x:@.session
            args
               function_waiting:bool:false
         sockets.signal:x:@.session
            args
               error:bool:true
               status:int:500
               message:Too many function invocations. Configure your type to handle more invocations or change your prompt.
         sockets.signal:x:@.session
            args
               finished:bool:true
   else
      execute:magic.ai.functions.invoke
         type:x:@.type
         session:x:@.session
         user-id:x:@.user-id
         invocation:x:@.dp/#
      lambda2json:x:@execute/*/result/*
         format:true
      if
         mt
            openai.tokenize:x:@lambda2json
            get-value:x:@.arguments/@.arguments/*/max_context_tokens
         .lambda
            throw:Result too large, try to limit your result
               status:int:400
               public:bool:true
      config.get:"magic:chat:functions:success-message"
         .:Success!
      unwrap:x:+/*/*
      sockets.signal:x:@.session
         args
            function_result:x:@config.get
            invocation:x:@execute/*/json
            file:x:@execute/*/workflow
      if
         exists:x:@execute/*/result/*
         .lambda
            set-value:x:@.new-prompt
               strings.concat
                  get-value:x:@.new-prompt
                  .:"Response from '"
                  get-value:x:@execute/*/workflow
                  .:@"' was:
```json
"
                  get-value:x:@lambda2json
                  .:@"
"
                  .:```
                  .:@"
"
                  .:@"
"
            set-value:x:@.function-result
               get-value:x:@lambda2json
      else
         set-value:x:@.new-prompt
            strings.concat
               get-value:x:@.new-prompt
               .:"Invocation of '"
               get-value:x:@execute/*/workflow
               .:"' was a success."
               .:@"
"
               .:@"
"
