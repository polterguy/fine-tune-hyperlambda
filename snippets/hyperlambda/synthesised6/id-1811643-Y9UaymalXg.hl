
// Compare the number of tokens required for the prompt (using [openai.tokenize] on [@.arguments/*/prompt]) with the maximum allowed request tokens (converted to integer from [@.model/*/max_request_tokens]) and return true if the prompt token count is more than the maximum allowed.
mt
   openai.tokenize:x:@.arguments/*/prompt
   convert:x:@.model/*/max_request_tokens
      type:int
