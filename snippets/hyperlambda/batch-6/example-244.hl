
/*
 * This Hyperlambda endpoint synthesizes training snippets from entries in the 
 * 'hyperlambda' table of the 'fine-tune' database, where each record has not yet 
 * been marked as 'synthesised' and does not contain [.arguments] in the [code] field.
 *
 * It generates a proper file-level comment using the [prompt] and merges it with 
 * the original [code], then sends the full document to OpenAI for enrichment before 
 * saving the result into a structured `.hl` file.
 *
 * 1. [.arguments]
 *    - Requires [limit:long] and [offset:long] to paginate the dataset.
 *    - These control how many records are retrieved and where the batch starts.
 *
 * 2. [.system-message]
 *    - Loads a static instruction file ('system-message-synthesiser.md') to guide OpenAI's behavior.
 *
 * 3. [data.connect:fine-tune] + [data.select]
 *    - Connects to the fine-tuning database and retrieves unsynthesized, non-static Hyperlambda entries.
 *    - The result is sorted by combined prompt + code length to prioritize larger entries.
 *
 * 4. [for-each]
 *    - Iterates over each returned training record.
 *
 * 5. [.filename]
 *    - Constructs a unique filename for the synthetic output using the record’s [hyperlambda_id].
 *    - Saves into the 'synthesised' subfolder under '/validation/' for better traceability.
 *
 * 6. [if not io.file.exists]
 *    - Ensures that no duplicate synthetic file is created for the same entry.
 *
 * 7. [hyper2lambda] + [insert-before]
 *    - Converts original [code] into a lambda object with comments.
 *    - Inserts the [prompt] as a file-level comment before the first lambda node.
 *
 * 8. [lambda2hyper]
 *    - Converts the modified lambda back into Hyperlambda syntax with comments.
 *    - Resulting text is used as both input to OpenAI and potential file content.
 *
 * 9. [log.info]
 *    - Logs which file is currently being processed and sent to OpenAI.
 *
 * 10. [execute:openai-context-from-database]
 *     - Retrieves contextual documentation related to the current snippet using semantic search.
 *     - Uses the full modified file content as the [query].
 *
 * 11. [execute:openai-query]
 *     - Sends prompt, system instruction, and documentation context to OpenAI.
 *     - Uses 'gpt-4o' model and limits total response to 1500 tokens.
 *     - OpenAI returns a new enriched training snippet or documentation.
 *
 * 12. [io.file.save]
 *     - Saves OpenAI’s output to disk at the generated filename.
 *
 * 13. [sleep:2000]
 *     - Waits 2 seconds between each request to avoid rate-limiting (429 errors).
 *
 * This endpoint is ideal for enhancing existing prompt/code pairs with enriched commentary, 
 * automated explanations, or contextual metadata — creating high-quality fine-tuning material 
 * from raw Hyperlambda logic.
 */
.arguments
   limit:long
   offset:long
.type:public

// Ensuring both arguments are specified by caller!
validators.mandatory:x:@.arguments/*/limit
validators.mandatory:x:@.arguments/*/offset

// Loading our system message file.
.system-message
set-value:x:@.system-message
   io.file.load:/modules/fine-tune-hyperlambda/system-message-synthesiser.md

// Connecting to our fine-tune database.
data.connect:fine-tune

   // Selecting records.
   data.select:"select * from hyperlambda where is_static = 0 and synthesised = 0 order by length(code) + length(prompt) limit @limit offset @offset"
      @limit:x:@.arguments/*/limit
      @offset:x:@.arguments/*/offset

   // Iterating through records.
   for-each:x:@data.select/*

      // Creating a unique filename
      .filename
      set-value:x:@.filename
         strings.concat
            .:/modules/fine-tune-hyperlambda/snippets/validation/synthesised/
            get-value:x:@.dp/#/*/hyperlambda_id
            .:.hl

      // Verifying examples doesn't already exist
      if
         not
            io.file.exists:x:@.filename
         .lambda

            // Transforming prompt + code into a file-level comment + original code.
            hyper2lambda:x:@.dp/#/*/code
               comments:true
            unwrap:x:+/*/*
            insert-before:x:@hyper2lambda/0
               .
                  ..:x:@.dp/#/*/prompt

            // Transforming to Hyperlambda again.
            .hl
            set-value:x:@.hl
               lambda2hyper:x:@hyper2lambda/*
                  comments:true

            // Doing some logging.
            log.info:Invoking OpenAI
               prompt:x:@.hl


            /*
             * Returns context for the specified [query] from the specified [type],
             * and returns to caller.
             *
             * Optionally add [threshold] and [max_tokens] to specify similarity and maximum
             * tokens to return. [type] is a machine learning type from your ml_types database table in
             * your magic database.
             */
            execute:magic.workflows.actions.execute
               name:openai-context-from-database
               filename:/modules/openai/workflows/actions/openai-context-from-database.hl
               arguments
                  type:magic-documentation
                  query:x:@.hl
                  threshold:decimal:0.3
                  max_tokens:int:3000


            /*
             * Invokes OpenAI's chat API with the specified [model], [max_tokens], [context],
             * [instruction] and [query].
             *
             * Will use the default API key found from configurations.
             *
             * [context] is additional data you can supply to OpenAI such that it responds using
             * your data as its source for information. [instruction] is a system message allowing
             * you to change how OpenAI responds.
             */
            execute:magic.workflows.actions.execute
               name:openai-query
               filename:/modules/openai/workflows/actions/openai-query.hl
               arguments
                  model:gpt-4o
                  max_tokens:int:1500
                  instruction:x:@.system-message
                  context:x:@execute/@execute/*/context
                  query:x:@.hl

            // Saving to file system.
            io.file.save:x:@.filename
               get-value:x:@execute/*/answer

            // To avoid 429 from OpenAI we wait for 5 seconds before we execute the next record.
            sleep:2000
