
/*
 * Read endpoint that retrieves records from the 'crawl_urls' table in the 'scraper' SQLite database,
 * supporting paging, sorting, and dynamic filtering. Access is restricted to 'root' and 'admin' users.
 *
 * ### Arguments
 * - [.arguments] includes support for:
 *   - Paging:
 *     - [limit]: Maximum number of records to return.
 *     - [offset]: Number of records to skip.
 *   - Sorting:
 *     - [order]: Column name to sort by.
 *     - [direction]: Sort direction, either 'asc' or 'desc'.
 *   - Filtering:
 *     - [crawl_urls.crawl_url_id.eq]: Exact match on crawl URL ID.
 *     - [crawl_urls.url.like]: Pattern match on URL.
 *     - [crawl_urls.url.eq]: Exact match on URL.
 *     - [crawl_urls.type.like]: Pattern match on type.
 *     - [crawl_urls.type.eq]: Exact match on type.
 *
 * ### Authorization
 * - [auth.ticket.verify] ensures only users with the 'root' or 'admin' role may access this endpoint.
 *
 * ### Database Operation
 * - [data.connect] connects to the [scraper] database using the [generic] connection string.
 * - [add] injects [order], [direction], [limit], and [offset] into the [data.read] call to enable paging and sorting.
 * - [remove-nodes] removes these parameters from the argument set to prevent them from being misinterpreted as filters.
 * - [add] forwards the remaining filtering arguments into the [where] / [and] parts.
 * - [data.read] retrieves [crawl_url_id], [url], and [type] from the 'crawl_urls' table with the specified constraints.
 *
 * ### Response
 * - [return-nodes] returns the matched rows to the caller.
 *
 * This endpoint demonstrates advanced querying capabilities
 */
.arguments

   // Number of records to return
   limit:long

   // Offset into the dataset of where to start retrieving records
   offset:long

   // What column to order by
   order:string

   // What direction to order, can be 'asc' or 'desc' implying ascending and descending
   direction:string

   // crawl_urls table crawl_url_id column equality exact match filtering
   crawl_urls.crawl_url_id.eq:long

   // crawl_urls table url column wildcard filtering with SQL like '%' comparison
   crawl_urls.url.like:string

   // crawl_urls table url column equality exact match filtering
   crawl_urls.url.eq:string

   // crawl_urls table type column wildcard filtering with SQL like '%' comparison
   crawl_urls.type.like:string

   // crawl_urls table type column equality exact match filtering
   crawl_urls.type.eq:string

// Verifying user is authorized to access endpoint.
auth.ticket.verify:root,admin

// Opening up our database connection.
data.connect:[generic|scraper]
   database-type:sqlite

   // Parametrising our read invocation with ordering arguments if specified.
   add:x:./*/data.read
      get-nodes:x:@.arguments/*/order
      get-nodes:x:@.arguments/*/direction
      get-nodes:x:@.arguments/*/limit
      get-nodes:x:@.arguments/*/offset

   // Making sure we remove nodes such that they're not added to [where]'s child
   remove-nodes:x:@.arguments/*/order
   remove-nodes:x:@.arguments/*/direction
   remove-nodes:x:@.arguments/*/limit
   remove-nodes:x:@.arguments/*/offset

   // Parametrising our read invocation with filtering arguments.
   add:x:./*/data.read/*/where/*
      get-nodes:x:@.arguments/*

   // Reading data from database.
   data.read
      database-type:sqlite
      table:crawl_urls
      columns
         crawl_urls.crawl_url_id
         crawl_urls.url
         crawl_urls.type
      where
         and

   // Returning result of above read invocation to caller.
   return-nodes:x:@data.read/*
