
// Checks if either max_context_tokens or max_request_tokens is missing in the model configuration, and if so, sets both values based on the model type by subtracting max_tokens from a model-specific constant, dividing the result by 2, and assigning this value to both max_context_tokens and max_request_tokens. Different constants are used depending on the specific model name.
if
   or
      null:x:@.model/*/max_context_tokens
      null:x:@.model/*/max_request_tokens
   .lambda
      switch:x:@.model/*/model
         case:text-davinci-003
         case:gpt-3.5-turbo
         case:gpt-3.5-turbo-0301
         case:text-davinci-002
            math.divide
               math.subtract:int:4096
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         case:code-davinci-002
            math.divide
               math.subtract:int:8000
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         case:gpt-4
         case:gpt-4-0314
         case:gpt-4o
         case:gpt-4o-2024-05-13
            math.divide
               math.subtract:int:8192
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         case:gpt-3.5-turbo-0125
            math.divide
               math.subtract:int:16384
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         case:gpt-4-32k
         case:gpt-4-32k-0314
            math.divide
               math.subtract:int:32768
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         case:gpt-4-0125-preview
         case:gpt-4-1106-preview
            math.divide
               math.subtract:int:131072
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         case:o3-mini
            math.divide
               math.subtract:int:200000
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         default
            math.divide
               math.subtract:int:2049
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
