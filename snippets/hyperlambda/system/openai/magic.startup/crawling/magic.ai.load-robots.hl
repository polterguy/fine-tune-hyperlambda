
/*
 * Loads the robots.txt file for the specified URL.
 */
slots.create:magic.ai.load-robots

   // Sanity checking invocation.
   validators.mandatory:x:@.arguments/*/url
   validators.url:x:@.arguments/*/url

   // Adding [headers] argument unless already specified.
   if
      not-exists:x:@.arguments/*/headers
      .lambda

         // Adding [headers] to [.arguments] such that we can create default HTTP headers further down.
         add:x:@.arguments
            .
               headers

   // Adding default headers unless they're already specified.
   validators.default:x:@.arguments/*/headers
      User-Agent:AINIRO-Crawler 2.0
      Accept-Encoding:identity
      Accept:text/plain

   // Removing everything but domain and scheme from URL.
   .domain
   strings.split:x:@.arguments/*/url
      .:/
   set-value:x:@.domain
      strings.concat
         get-value:x:@strings.split/0
         .://
         get-value:x:@strings.split/1

   /*
    * This will contain the URL to all sitemaps of the site, if there exists sitemaps.
    *
    * Notice, we're defaulting sitemap URL to a single https://x.com/sitemap.xml unless explicitly
    * overridden in robots.txt file.
    */
   .sitemap

   // Creating our robots.txt URL.
   .robots
   set-value:x:@.robots
      strings.concat
         get-value:x:@.domain
         .:/robots.txt
   add:x:./*/http.get
      get-nodes:x:@.arguments/*/headers

   // Informing user that we're trying to retrieve robots.txt.
   strings.concat
      .:"Trying to retrieve robots.txt file from "
      get-value:x:@.robots
   unwrap:x:+/**
   sockets.signal:x:@.arguments/*/feedback-channel
      args
         message:x:@strings.concat
         type:info

   // Making sure we trap exceptions.
   try

      // Retrieving robots.txt file.
      http.get:x:@.robots
         timeout:60

      // Collection of URL explicitly disallowed from robots.txt file.
      .disallow
      .allow
      .crawl-delay

      // True if we found a valid robots.txt file.
      .has-robots:bool:false

      // Checking if above invocation succeeded.
      if
         and
            mte:x:@http.get
               .:int:200
            lt:x:@http.get
               .:int:300
         .lambda

            // Verifying request returned text/plain MIME type.
            if
               or
                  strings.starts-with:x:@http.get/*/headers/*/Content-Type
                     .:text/plain
                  strings.starts-with:x:@http.get/*/headers/*/content-type
                     .:text/plain
               .lambda

                  // We've got a robots.txt file, and it's a text file.
                  set-value:x:@.has-robots
                     .:bool:true

                  // Splitting file into separate lines.
                  strings.split:x:@http.get/*/content
                     .:"\n"

                  /*
                   * If true it implies we're in a section of the robots.txt file
                   * relevant for AINIRO crawler.
                   */
                  .relevant:bool:false

                  // True if we've seen an explicit AINIRO crawler section.
                  .has-seen-ainiro:bool:false

                  // Looping through each line from above invocation.
                  for-each:x:@strings.split/*

                     // Trimming each line.
                     strings.trim:x:@.dp/#
                        .:"\r\n\t "

                     // Making sure this is not a comment.
                     if
                        not
                           strings.starts-with:x:@strings.trim
                              .:#
                        .lambda

                           // splitting up line into name/value pair.
                           strings.split:x:@strings.trim
                              .:":"

                           // Verifying this is a name/value pair.
                           if
                              mte
                                 get-count:x:@strings.split/*
                                 .:int:2
                              .lambda

                                 // Recombining splits, except first part.
                                 .value
                                 set-value:x:@.value
                                    strings.join:x:@strings.split/*/[1,100]
                                       .:":"

                                 // Trimming name and value.
                                 set-value:x:@strings.split/0
                                    strings.trim:x:@strings.split/0
                                       .:"\r\n\t "
                                 set-value:x:@.value
                                    strings.trim:x:@.value
                                       .:"\r\n\t "

                                 // Checking what type of value this is.
                                 switch:x:@strings.split/0

                                    case:Crawl-Delay
                                    case:Crawl-delay
                                    case:crawl-delay

                                       // Replacing everything that's not a digit in value.
                                       set-value:x:@.value
                                          strings.replace-not-of:x:@.value
                                             .:0123456789
                                             .:

                                       // Checking if we're inside a relevant section from the robots.txt file.
                                       if
                                          and
                                             eq:x:@.relevant
                                                .:bool:true
                                             neq:x:@.value
                                                .:
                                          .lambda

                                             // In case Crawl-Delay is not a number we need to catch exceptions.
                                             try

                                                // Storing Crawl-Delay value and returning to caller.
                                                set-value:x:@.crawl-delay
                                                   math.multiply
                                                      .:int:1000
                                                      convert:x:@.value
                                                         type:int

                                                // Verifying Crawl-Delay is not more than 60000.
                                                if
                                                   mt:x:@.crawl-delay
                                                      .:int:60000
                                                   .lambda

                                                      // Crawl-Delay more than 60 seconds, warning user.
                                                      sockets.signal:x:@.arguments/*/feedback-channel
                                                         args
                                                            message:Crawl-Delay was more than 60 seconds, using 60 seconds as our value
                                                            type:warning
                                                      set-value:x:@.crawl-delay
                                                         .:int:60000

                                             .catch

                                                // Crawl-Delay was not a number, warning user.
                                                sockets.signal:x:@.arguments/*/feedback-channel
                                                   args
                                                      message:Crawl-Delay was not a valid integer value, using 10 seconds as our value
                                                      type:warning
                                                set-value:x:@.crawl-delay
                                                   .:int:10000

                                    case:User-Agent
                                    case:User-agent
                                    case:user-agent

                                       // Checking if section is relevant for us.
                                       switch:x:@.value

                                          // These are our interesting sections.
                                          case:*
                                          case:AINIRO
                                          case:GPTBot

                                             /*
                                              * Checking if this is explicit AINIRO section, and if so,
                                              * clearing allows/disallows.
                                              */
                                             if
                                                eq:x:@.value
                                                   .:AINIRO
                                                .lambda

                                                   // Removing previously iterated allow/disallows.
                                                   remove-nodes:x:@.allow/*
                                                   remove-nodes:x:@.disallow/*
                                                   set-value:x:@.has-seen-ainiro
                                                      .:bool:true

                                                   // Section is relevant for us.
                                                   set-value:x:@.relevant
                                                      .:bool:true

                                             else

                                                /*
                                                 * Making sure we've not seen an explicit AINIRO section
                                                 * before setting [.relevant].
                                                 */
                                                if
                                                   eq:x:@.has-seen-ainiro
                                                      .:bool:false
                                                   .lambda

                                                      // Section is relevant for us.
                                                      set-value:x:@.relevant
                                                         .:bool:true
                                                else

                                                   /*
                                                    * Section is relevant for us because we've already seen
                                                    * AINIRO specific section.
                                                    */
                                                   set-value:x:@.relevant
                                                      .:bool:false

                                          default

                                             // Section is not relevant for us.
                                             set-value:x:@.relevant
                                                .:bool:false

                                    case:Sitemap
                                    case:sitemap

                                       // Updating above [.sitemap], but we need to check if sitemap URL is relative.
                                       if
                                          or
                                             strings.starts-with:x:@.value
                                                .:"http://"
                                             strings.starts-with:x:@.value
                                                .:"https://"
                                          .lambda

                                             // Absolute URL.
                                             unwrap:x:+/*/*
                                             add:x:@.sitemap
                                                .
                                                   .:x:@.value

                                       else

                                          // URL is relative.
                                          strings.concat
                                             get-value:x:@.domain
                                             .:/
                                             get-value:x:@.value
                                          strings.replace:x:@strings.concat
                                             .://
                                             .:/
                                          unwrap:x:+/*/*
                                          add:x:@.sitemap
                                             .
                                                .:x:@strings.replace

                                    case:Disallow
                                    case:disallow

                                       // Checking if we're inside a relevant section from the robots.txt file.
                                       if
                                          eq:x:@.relevant
                                             .:bool:true
                                          .lambda

                                             // Updating above [.disallow], but we need to check if URL is relative first.
                                             if
                                                or
                                                   strings.starts-with:x:@.value
                                                      .:"http://"
                                                   strings.starts-with:x:@.value
                                                      .:"https://"
                                                .lambda

                                                   // Absolute URL.
                                                   unwrap:x:+/*/*
                                                   add:x:@.disallow
                                                      .
                                                         .:x:@.value

                                             else

                                                // URL is relative.
                                                strings.concat
                                                   get-value:x:@.domain
                                                   get-value:x:@.value
                                                unwrap:x:+/*/*
                                                add:x:@.disallow
                                                   .
                                                      .:x:@strings.concat

                                    case:Allow
                                    case:allow

                                       // Checking if we're inside a relevant section from the robots.txt file.
                                       if
                                          eq:x:@.relevant
                                             .:bool:true
                                          .lambda

                                             // Updating above [.disallow], but we need to check if URL is relative first.
                                             if
                                                or
                                                   strings.starts-with:x:@.value
                                                      .:"http://"
                                                   strings.starts-with:x:@.value
                                                      .:"https://"
                                                .lambda

                                                   // Absolute URL.
                                                   unwrap:x:+/*/*
                                                   add:x:@.allow
                                                      .
                                                         .:x:@.value

                                             else

                                                // URL is relative.
                                                strings.concat
                                                   get-value:x:@.domain
                                                   get-value:x:@.value
                                                unwrap:x:+/*/*
                                                add:x:@.allow
                                                   .
                                                      .:x:@strings.concat

      // Returning disallowed URLs to caller.
      if
         exists:x:@.disallow/*
         .lambda

            // We've got disallowed URLs in robots.txt file.
            add:x:+/*/*
               get-nodes:x:@.disallow/*
            add:x:@try/*/return
               .
                  disallow

      // Returning allowed URLs to caller.
      if
         exists:x:@.allow/*
         .lambda

            // We've got allowed URLs in robots.txt file.
            add:x:+/*/*
               get-nodes:x:@.allow/*
            add:x:@try/*/return
               .
                  allow

      // Returning allowed sitemap URLs to caller.
      if
         exists:x:@.sitemap/*
         .lambda

            /*
             * Sorting sitemap URLs by length, which typically makes sure more
             * important sitemaps are prioritised.
             */
            sort:x:@.sitemap/*
               if
                  lt
                     strings.length:x:@.lhs/#
                     strings.length:x:@.rhs/#
                  .lambda
                     set-value:x:@.result
                        .:int:-1
               else-if
                  mt
                     strings.length:x:@.lhs/#
                     strings.length:x:@.rhs/#
                  .lambda
                     set-value:x:@.result
                        .:int:1
               else
                  set-value:x:@.result
                     .:int:0

            // Adding sorted sitemaps to [return].
            add:x:+/*/*
               get-nodes:x:@sort/*
            add:x:@try/*/return
               .
                  sitemap

      else

         /*
          * If we could not find any sitemaps,
          * we return x.com/sitemap.xml as the default sitemap such that
          * caller can try to retrieve it in case it exists.
          */
         strings.concat
            get-value:x:@.domain
            .:/sitemap.xml
         unwrap:x:+/*/*/*
         add:x:@try/*/return
            .
               sitemap
                  .:x:@strings.concat

      // Checking if we've got a crawl-delay value
      if
         not-null:x:@.crawl-delay
         .lambda

            // Making sure we return crawl-delay.
            unwrap:x:+/*/*
            add:x:@try/*/return
               .
                  crawl-delay:x:@.crawl-delay

      // Returning result to caller.
      unwrap:x:+/*
      return
         found:x:@.has-robots

   .catch

      // Informing user that something went wrong.
      strings.concat
         .:"Error while trying to retrieve robots.txt, message was; '"
         get-value:x:@.arguments/*/message
         .:"'"
      unwrap:x:+/**
      sockets.signal:x:@.arguments/@.arguments/*/feedback-channel
         args
            message:x:@strings.concat
            type:warning

      // Returning "no robots" to caller.
      strings.concat
         get-value:x:@.domain
         .:/sitemap.xml
      unwrap:x:+/*/*
      return
         found:bool:false
         sitemap
            .:x:@strings.concat
