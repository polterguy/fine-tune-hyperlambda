
/*
 * This Hyperlambda code uses a [switch] statement to determine the model type from the [.model] node and calculates the maximum context and request tokens based on the model's maximum tokens. Each [case] corresponds to a specific model type, and for each case, it subtracts the model's maximum tokens from a predefined integer, divides the result by 2, and assigns these values to [.model/*/max_context_tokens] and [.model/*/max_request_tokens]. If the model type does not match any case, the [default] case uses a base value of 2049 for calculations. The code ensures that the token limits are adjusted based on the model's capabilities. 
 * 
 * 1. [switch] - Determines the model type from [.model/*/model].
 * 2. [case] - Specifies different model types to handle.
 * 3. [math.divide] - Divides the result of the subtraction by 2.
 * 4. [math.subtract] - Subtracts the model's max tokens from a predefined integer.
 * 5. [get-value] - Retrieves the value of the [.model/*/max_tokens].
 * 6. [set-value] - Assigns the calculated value to [.model/*/max_context_tokens] and [.model/*/max_request_tokens].
 * 7. [default] - Handles cases where the model type does not match any specified case.
 */
switch:x:@.model/*/model
   case:text-davinci-003
   case:gpt-3.5-turbo
   case:gpt-3.5-turbo-0301
   case:text-davinci-002
      math.divide
         math.subtract:int:4096
            get-value:x:@.model/*/max_tokens
         .:int:2
      set-value:x:@.model/*/max_context_tokens
         get-value:x:@math.divide
      set-value:x:@.model/*/max_request_tokens
         get-value:x:@math.divide
   case:code-davinci-002
      math.divide
         math.subtract:int:8000
            get-value:x:@.model/*/max_tokens
         .:int:2
      set-value:x:@.model/*/max_context_tokens
         get-value:x:@math.divide
      set-value:x:@.model/*/max_request_tokens
         get-value:x:@math.divide
   case:gpt-4
   case:gpt-4-0314
   case:gpt-4o
   case:gpt-4o-2024-05-13
      math.divide
         math.subtract:int:8192
            get-value:x:@.model/*/max_tokens
         .:int:2
      set-value:x:@.model/*/max_context_tokens
         get-value:x:@math.divide
      set-value:x:@.model/*/max_request_tokens
         get-value:x:@math.divide
   case:gpt-3.5-turbo-0125
      math.divide
         math.subtract:int:16384
            get-value:x:@.model/*/max_tokens
         .:int:2
      set-value:x:@.model/*/max_context_tokens
         get-value:x:@math.divide
      set-value:x:@.model/*/max_request_tokens
         get-value:x:@math.divide
   case:gpt-4-32k
   case:gpt-4-32k-0314
      math.divide
         math.subtract:int:32768
            get-value:x:@.model/*/max_tokens
         .:int:2
      set-value:x:@.model/*/max_context_tokens
         get-value:x:@math.divide
      set-value:x:@.model/*/max_request_tokens
         get-value:x:@math.divide
   case:gpt-4-0125-preview
   case:gpt-4-1106-preview
      math.divide
         math.subtract:int:131072
            get-value:x:@.model/*/max_tokens
         .:int:2
      set-value:x:@.model/*/max_context_tokens
         get-value:x:@math.divide
      set-value:x:@.model/*/max_request_tokens
         get-value:x:@math.divide
   case:o3-mini
      math.divide
         math.subtract:int:200000
            get-value:x:@.model/*/max_tokens
         .:int:2
      set-value:x:@.model/*/max_context_tokens
         get-value:x:@math.divide
      set-value:x:@.model/*/max_request_tokens
         get-value:x:@math.divide
   default
      math.divide
         math.subtract:int:2049
            get-value:x:@.model/*/max_tokens
         .:int:2
      set-value:x:@.model/*/max_context_tokens
         get-value:x:@math.divide
      set-value:x:@.model/*/max_request_tokens
         get-value:x:@math.divide
