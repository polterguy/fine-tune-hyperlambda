
/*
 * This Hyperlambda code compares the number of tokens required for a given prompt against the maximum request tokens allowed by a model. It uses the [openai.tokenize] slot to calculate the token count of the prompt and the [convert] slot to ensure the maximum token limit is an integer. The [mt] slot then checks if the token count of the prompt is more than the maximum allowed tokens. 
 * 
 * 1. [mt] - Compares two values and returns true if the first is more than the second.
 * 2. [openai.tokenize] - Calculates the number of tokens for the given prompt.
 * 3. [convert] - Converts the maximum request tokens value to an integer type.
 * 4. [type] - Specifies the conversion type as an integer.
 */
mt
   openai.tokenize:x:@.arguments/*/prompt
   convert:x:@.model/*/max_request_tokens
      type:int
