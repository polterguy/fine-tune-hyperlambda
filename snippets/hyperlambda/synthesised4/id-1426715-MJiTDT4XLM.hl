
/*
 * This Hyperlambda code defines a dynamic slot [magic.ai.endpoint-common] that serves as a common endpoint for AI model interactions. It validates input arguments, retrieves model configuration from a database, and performs authentication and authorization checks. The slot adjusts model parameters based on predefined conditions, handles incoming webhooks, and ensures request constraints are met. It calculates token usage for the request and updates request counts in the database. Finally, it invokes a callback function and returns the results. Key slots and their functions include:
 * 
 * 1. [validators.mandatory] - Ensures required arguments are provided.
 * 2. [strings.trim] - Trims whitespace from the prompt.
 * 3. [data.connect] - Connects to a database to retrieve model configurations.
 * 4. [if] - Implements conditional logic for various checks and operations.
 * 5. [auth.ticket.verify] - Verifies authentication tickets.
 * 6. [validators.recaptcha] - Validates reCAPTCHA responses.
 * 7. [math.divide] - Performs mathematical operations for token calculations.
 * 8. [switch] - Adjusts model parameters based on model type.
 * 9. [openai.tokenize] - Calculates token usage for the request.
 * 10. [invoke] - Executes a callback function.
 * 11. [data.execute] - Updates request counts in the database.
 * 12. [return-nodes] - Returns the results of the invocation.
 */
slots.create:magic.ai.endpoint-common
   validators.mandatory:x:@.arguments/*/type
   validators.mandatory:x:@.arguments/*/prompt
   validators.string:x:@.arguments/*/prompt
      min:1
   set-value:x:@.arguments/*/prompt
      strings.trim:x:@.arguments/*/prompt
   .model
   data.connect:[generic|magic]
      data.read
         table:ml_types
         columns
            model
            max_tokens
            max_context_tokens
            max_request_tokens
            temperature
            recaptcha
            auth
            supervised
            cached
            prefix
            use_embeddings
            threshold
            vector_model
            contact_us
            lead_email
            api_key
            webhook_incoming
            webhook_outgoing
            webhook_incoming_url
            webhook_outgoing_url
            system_message
            initial_questionnaire
            no_requests
            max_requests
            session_timeout
            search_postfix
            max_function_invocations
            max_session_items
            completion_slot
         where
            and
               type.eq:x:@.arguments/*/type
      if
         not-exists:x:@data.read/*
         .lambda
            data.read
               table:ml_types
               columns
                  model
                  max_tokens
                  max_context_tokens
                  max_request_tokens
                  temperature
                  recaptcha
                  auth
                  supervised
                  cached
                  prefix
                  use_embeddings
                  threshold
                  vector_model
                  contact_us
                  lead_email
                  api_key
                  webhook_incoming
                  webhook_outgoing
                  webhook_incoming_url
                  webhook_outgoing_url
                  system_message
                  initial_questionnaire
                  no_requests
                  max_requests
                  session_timeout
                  search_postfix
                  max_function_invocations
                  max_session_items
                  completion_slot
               where
                  and
                     type.eq:default
            if
               not-exists:x:@data.read/*
               .lambda
                  throw:No such type, and no default type was found
                     status:int:400
                     public:bool:true
            add:x:@.model
               get-nodes:x:@data.read/*/*
            set-value:x:@.arguments/*/type
               .:default
      else
         add:x:@.model
            get-nodes:x:@data.read/*/*
   if
      or
         not-exists:x:@.arguments/*/skip-auth
         eq:x:@.arguments/*/skip-auth
            .:bool:false
      .lambda
         if
            and
               not-null:x:@.model/*/auth
               neq:x:@.model/*/auth
                  .:
            .lambda
               auth.ticket.verify:x:@.model/*/auth
         if
            and
               eq
                  auth.ticket.get
                  .
               mt
                  convert:x:@.model/*/recaptcha
                     type:decimal
                  .:decimal:0
               not-null:x:@.model/*/recaptcha
            .lambda
               if
                  or
                     not-exists:x:@.arguments/*/recaptcha_response
                     null:x:@.arguments/*/recaptcha_response
                  .lambda
                     response.status.set:499
                     return
                        error:No reCAPTCHA supplied
               .key
               set-value:x:@.key
                  config.get:"magic:auth:recaptcha:key"
               .secret
               set-value:x:@.secret
                  config.get:"magic:auth:recaptcha:secret"
               convert:x:@.model/*/recaptcha
                  type:decimal
               validators.recaptcha:x:@.arguments/*/recaptcha_response
                  min:x:@convert
                  site-key:x:@.key
                  secret:x:@.secret
         else-if
            and
               eq
                  auth.ticket.get
                  .
               not
                  lt
                     convert:x:@.model/*/recaptcha
                        type:decimal
                     .:decimal:0
            .lambda
               execute:magic.auth.captcha-verify
                  token:x:@.arguments/*/recaptcha_response
   if
      and
         neq:x:@.model/*/max_requests
            .:long:-1
         mte
            get-value:x:@.model/*/no_requests
            get-value:x:@.model/*/max_requests
      .lambda
         throw:Maximum number of requests reached for your model this month
            public:bool:true
            status:int:500
   if
      strings.contains:x:@.model/*/system_message
         .:{{
      .lambda
         set-value:x:@.model/*/system_message
            strings.mixin:x:@.model/*/system_message
   .incoming
   set-value:x:@.incoming
      get-first-value
         get-value:x:@.model/*/webhook_incoming
         config.get:"magic:openai:integrations:incoming:slot"
   if
      and
         not-null:x:@.incoming
         neq:x:@.incoming
            .:
      .lambda
         .exe
            .hook-url
            set-value:x:@.hook-url
               get-first-value
                  get-value:x:@.model/*/webhook_incoming_url
                  config.get:"magic:openai:integrations:incoming:url"
            unwrap:x:./*/signal/*/url
            signal:x:@.incoming
               url:x:@.hook-url
         if
            and
               exists:x:@.arguments/*/to
               exists:x:@.arguments/*/from
               not-null:x:@.arguments/*/to
               not-null:x:@.arguments/*/from
               strings.contains:x:@.arguments/*/to
                  .:":"
               strings.contains:x:@.arguments/*/from
                  .:":"
            .lambda
               .channel
               .to
               .from
               strings.split:x:@.arguments/*/to
                  .:":"
               set-value:x:@.channel
                  get-value:x:@strings.split/0
               set-value:x:@.to
                  get-value:x:@strings.split/1
               strings.split:x:@.arguments/*/from
                  .:":"
               set-value:x:@.from
                  get-value:x:@strings.split/1
               add:x:@.exe/*/signal
                  get-nodes:x:@.arguments/*/prompt
                  get-nodes:x:@.arguments/*/session
               unwrap:x:+/*/*
               add:x:@.exe/*/signal
                  .
                     to:x:@.to
                     from:x:@.from
                     channel:x:@.channel
         else
            add:x:@.exe/*/signal
               get-nodes:x:@.arguments/*/to
               get-nodes:x:@.arguments/*/from
               get-nodes:x:@.arguments/*/prompt
               get-nodes:x:@.arguments/*/session
         eval:x:@.exe
   set-value:x:@.model/*/threshold
      convert:x:@.model/*/threshold
         type:double
   set-value:x:@.model/*/max_tokens
      convert:x:@.model/*/max_tokens
         type:int
   set-value:x:@.model/*/max_context_tokens
      convert:x:@.model/*/max_context_tokens
         type:int
   set-value:x:@.model/*/max_request_tokens
      convert:x:@.model/*/max_request_tokens
         type:int
   switch:x:@.model/*/model
      case:gpt-4o
      case:gpt-4o-mini
      case:gpt-4o-2024-05-13
      case:gpt-4-turbo
      case:gpt-4-0125-preview
      case:gpt-4-1106-preview
      case:gpt-4-turbo-preview
      case:gpt-4-turbo-2024-04-09
      case:gpt-4-vision-preview
      case:gpt-4-1106-vision-preview
         add:x:@.model
            .
               model_size:int:128000
      case:gpt-4
      case:gpt-4-0613
         add:x:@.model
            .
               model_size:int:8192
      case:gpt-4-32k
      case:gpt-4-32k-0613
         add:x:@.model
            .
               model_size:int:32768
      case:gpt-3.5-turbo
      case:gpt-3.5-turbo-0125
      case:gpt-3.5-turbo-1106
      case:gpt-3.5-turbo-16k
      case:gpt-3.5-turbo-16k-0613
         add:x:@.model
            .
               model_size:int:16384
      case:gpt-3.5-turbo-0613
      case:gpt-3.5-turbo-instruct
         add:x:@.model
            .
               model_size:int:4096
      case:o3-mini
         add:x:@.model
            .
               model_size:int:200000
      case:gpt-4.5-preview
         add:x:@.model
            .
               model_size:int:128000
   if
      or
         null:x:@.model/*/max_context_tokens
         null:x:@.model/*/max_request_tokens
      .lambda
         switch:x:@.model/*/model
            case:text-davinci-003
            case:gpt-3.5-turbo
            case:gpt-3.5-turbo-0301
            case:text-davinci-002
               math.divide
                  math.subtract:int:4096
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            case:code-davinci-002
               math.divide
                  math.subtract:int:8000
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            case:gpt-4
            case:gpt-4-0314
            case:gpt-4o
            case:gpt-4o-2024-05-13
               math.divide
                  math.subtract:int:8192
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            case:gpt-3.5-turbo-0125
               math.divide
                  math.subtract:int:16384
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            case:gpt-4-32k
            case:gpt-4-32k-0314
               math.divide
                  math.subtract:int:32768
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            case:gpt-4-0125-preview
            case:gpt-4-1106-preview
               math.divide
                  math.subtract:int:131072
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            case:o3-mini
               math.divide
                  math.subtract:int:200000
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
            default
               math.divide
                  math.subtract:int:2049
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide
   if
      mt
         openai.tokenize:x:@.arguments/*/prompt
         convert:x:@.model/*/max_request_tokens
            type:int
      .lambda
         throw:Your request is longer than what this type is configured to allow for
            public:bool:true
            status:int:400
   add:x:./*/invoke
      get-nodes:x:@.arguments/*
      get-nodes:x:@.model/*
   remove-nodes:x:./*/invoke/*/.callback
   invoke:x:@.arguments/*/.callback
   data.connect:[generic|magic]
      data.execute:update ml_types set no_requests = no_requests + 1 where type = @type
         type:x:@.arguments/*/type
   return-nodes:x:@invoke/*
