
/*
 * This Hyperlambda code checks if the number of tokens required for a given prompt exceeds the maximum allowed tokens for a specific model. If the prompt's token count is greater than the model's maximum request tokens, it throws an exception with a public message and a status code of 400.
 * 
 * 1. [if] - Begins a conditional statement.
 * 2. [mt] - Compares two values to check if the first is greater than the second.
 * 3. [openai.tokenize] - Calculates the number of tokens required for the prompt.
 * 4. [convert] - Retrieves the maximum allowed tokens for the model.
 * 5. [.lambda] - Contains the code to execute if the condition is true.
 * 6. [throw] - Throws an exception with a specified message and status code.
 * 7. [public] - Sets the exception message to be public.
 * 8. [status] - Sets the HTTP status code for the exception.
 */
if
   mt
      openai.tokenize:x:@.arguments/*/prompt
      convert:x:@.model/*/max_request_tokens
         type:int
   .lambda
      throw:Your request is longer than what this type is configured to allow for
         public:bool:true
         status:int:400
