
// This Hyperlambda code checks if the token count of a given prompt exceeds the maximum allowed tokens for a specific model. It uses [mt] to compare the token count from [openai.tokenize] with the model's maximum tokens from [convert]. If the prompt is too long, it throws an exception using [throw], making the error public with a status code of 400.
if
   mt
      openai.tokenize:x:@.arguments/*/prompt
      convert:x:@.model/*/max_request_tokens
         type:int
   .lambda
      throw:Your request is longer than what this type is configured to allow for
         public:bool:true
         status:int:400
