
/*
 * // This Hyperlambda code is part of a web scraping process that handles cases where no valid sitemaps are found. It attempts to crawl the site directly, signaling progress and status updates through a feedback channel. The process continues to scrape URLs while conditions are met, and it logs the completion of the scraping process. If a callback is provided, it will execute it after the scraping is done.
 * // 1. [else] - Begins the else block for handling cases where no valid sitemaps are found.
 * // 2. [.urls] - Initializes an empty node to store URLs to be scraped.
 * // 3. [.done] - Initializes an empty node to store URLs that have been successfully scraped.
 * // 4. [unwrap] - Unwraps all nodes to prepare for processing.
 * // 5. [add] - Adds the initial URL to the list of URLs to be scraped.
 * // 6. [sockets.signal] - Sends a warning message to the feedback channel indicating no valid sitemaps were found.
 * // 7. [sleep] - Pauses execution for 100 milliseconds.
 * // 8. [sockets.signal] - Sends an info message to the feedback channel indicating the site will be crawled despite no valid sitemap.
 * // 9. [sleep] - Pauses execution for 100 milliseconds.
 * // 10. [while] - Begins a loop that continues as long as there are URLs to scrape and the maximum number of URLs has not been reached.
 * // 11. [and] - Logical AND condition for the while loop.
 * // 12. [exists] - Checks if there are URLs left to scrape.
 * // 13. [lt] - Checks if the number of scraped URLs is less than the maximum allowed.
 * // 14. [.lambda] - The lambda block executed for each iteration of the while loop.
 * // 15. [sockets.signal] - Sends a separator message to the feedback channel.
 * // 16. [sleep] - Pauses execution for 100 milliseconds.
 * // 17. [unwrap] - Unwraps all nodes to prepare for processing.
 * // 18. [signal] - Sends a signal to scrape the next URL with specified parameters.
 * // 19. [add] - Adds the scraped URL to the list of done URLs.
 * // 20. [remove-nodes] - Removes the scraped URL from the list of URLs to be scraped.
 * // 21. [for-each] - Iterates over the results of the scraping signal.
 * // 22. [if] - Checks conditions to add new URLs to the list of URLs to be scraped.
 * // 23. [and] - Logical AND condition for the if statement.
 * // 24. [not-exists] - Checks if the URL is not already in the done or URLs list.
 * // 25. [strings.starts-with] - Checks if the URL starts with the initial URL.
 * // 26. [.lambda] - The lambda block executed if the conditions are met.
 * // 27. [add] - Adds new URLs to the list of URLs to be scraped.
 * // 28. [strings.concat] - Concatenates a message about waiting to avoid server exhaustion.
 * // 29. [unwrap] - Unwraps all nodes to prepare for processing.
 * // 30. [sockets.signal] - Sends an info message to the feedback channel with the wait message.
 * // 31. [sleep] - Pauses execution for 100 milliseconds.
 * // 32. [sleep] - Pauses execution for the specified delay.
 * // 33. [sockets.signal] - Sends a separator message to the feedback channel.
 * // 34. [sleep] - Pauses execution for 100 milliseconds.
 * // 35. [strings.concat] - Concatenates a message about the number of URLs scraped.
 * // 36. [unwrap] - Unwraps all nodes to prepare for processing.
 * // 37. [sockets.signal] - Sends an info message to the feedback channel with the completion message.
 * // 38. [sleep] - Pauses execution for 100 milliseconds.
 * // 39. [log.info] - Logs a message indicating the successful creation of OpenAI training data.
 * // 40. [if] - Checks if an onafter callback is provided.
 * // 41. [eval] - Executes the onafter callback if it exists.
 */
else
   .urls
   .done
   unwrap:x:+/*/*
   add:x:@.urls
      .
         .:x:@.arguments/*/url
   sockets.signal:x:@.arguments/*/feedback-channel
      args
         message:Could not find any valid sitemaps
         type:warning
   sleep:100
   sockets.signal:x:@.arguments/*/feedback-channel
      args
         message:Trying to crawl site even though we did not find a valid sitemap
         type:info
   sleep:100
   while
      and
         exists:x:@.urls/*
         lt
            get-count:x:@.done/*
            get-value:x:@.arguments/*/max
      .lambda
         sockets.signal:x:@.arguments/*/feedback-channel
            args
               message:------------------------------------------------------------------------------------------------------------------------
               type:info
         sleep:100
         unwrap:x:+/*
         signal:magic.ai.url.scrape
            url:x:@.urls/0
            type:x:@.arguments/*/type
            images:bool:true
            code:bool:true
            lists:bool:true
            main:bool:true
            empty-completion:bool:false
            threshold:x:@.arguments/*/threshold
            summarize:x:@.arguments/*/summarize
            feedback-channel:x:@.arguments/*/feedback-channel
         add:x:@.done
            get-nodes:x:@.urls/0
         remove-nodes:x:@.urls/0
         for-each:x:@signal/*
            if
               and
                  not-exists:x:"@.done/*/\"={@.dp/#}\""
                  not-exists:x:"@.urls/*/\"={@.dp/#}\""
                  strings.starts-with:x:@.dp/#
                     get-value:x:@.arguments/*/url
               .lambda
                  add:x:@.urls
                     get-nodes:x:@.dp/#
         strings.concat
            .:"Waiting for "
            math.divide:x:@.arguments/*/delay
               .:int:1000
            .:" seconds to avoid exhausting web server"
         unwrap:x:+/**
         sockets.signal:x:@.arguments/*/feedback-channel
            args
               message:x:@strings.concat
               type:info
         sleep:100
         sleep:x:@.arguments/*/delay
   sockets.signal:x:@.arguments/*/feedback-channel
      args
         message:------------------------------------------------------------------------------------------------------------------------
         type:info
   sleep:100
   strings.concat
      .:"Done scraping "
      get-count:x:@.done/*
      .:" URLs"
   unwrap:x:+/**
   sockets.signal:x:@.arguments/*/feedback-channel
      args
         message:x:@strings.concat
         type:info
   sleep:100
   log.info:OpenAI training data successfully created
      url:x:@.arguments/*/url
      type:x:@.arguments/*/type
   if
      exists:x:@.arguments/*/.onafter
      .lambda
         eval:x:@.arguments/*/.onafter
