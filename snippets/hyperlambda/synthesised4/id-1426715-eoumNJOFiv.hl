
/*
 * // This Hyperlambda code checks if either [max_context_tokens] or [max_request_tokens] is null for a given model. If so, it uses a [switch] statement to determine the model type and calculates the maximum context and request tokens based on predefined limits for each model. It subtracts half of the [max_tokens] from the model-specific limit and assigns the result to both [max_context_tokens] and [max_request_tokens]. If the model type does not match any case, it uses a default value of 2049 for calculations.
 * // 1. [if] - Checks if either [max_context_tokens] or [max_request_tokens] is null.
 * // 2. [or] - Logical OR to evaluate the nullity of the tokens.
 * // 3. [null:x:@.model/*/max_context_tokens] - Checks if [max_context_tokens] is null.
 * // 4. [null:x:@.model/*/max_request_tokens] - Checks if [max_request_tokens] is null.
 * // 5. [.lambda] - Executes the enclosed code if the [if] condition is true.
 * // 6. [switch:x:@.model/*/model] - Switches based on the model type.
 * // 7. [case:text-davinci-003] to [case:o3-mini] - Specifies different models to handle.
 * // 8. [math.divide] - Divides the result of [math.subtract] by 2.
 * // 9. [math.subtract:int:4096] (and other values) - Subtracts [max_tokens] from a model-specific limit.
 * // 10. [get-value:x:@.model/*/max_tokens] - Retrieves the value of [max_tokens].
 * // 11. [set-value:x:@.model/*/max_context_tokens] - Sets the calculated value to [max_context_tokens].
 * // 12. [set-value:x:@.model/*/max_request_tokens] - Sets the calculated value to [max_request_tokens].
 * // 13. [default] - Default case for models not explicitly handled.
 */
if
   or
      null:x:@.model/*/max_context_tokens
      null:x:@.model/*/max_request_tokens
   .lambda
      switch:x:@.model/*/model
         case:text-davinci-003
         case:gpt-3.5-turbo
         case:gpt-3.5-turbo-0301
         case:text-davinci-002
            math.divide
               math.subtract:int:4096
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         case:code-davinci-002
            math.divide
               math.subtract:int:8000
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         case:gpt-4
         case:gpt-4-0314
         case:gpt-4o
         case:gpt-4o-2024-05-13
            math.divide
               math.subtract:int:8192
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         case:gpt-3.5-turbo-0125
            math.divide
               math.subtract:int:16384
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         case:gpt-4-32k
         case:gpt-4-32k-0314
            math.divide
               math.subtract:int:32768
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         case:gpt-4-0125-preview
         case:gpt-4-1106-preview
            math.divide
               math.subtract:int:131072
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         case:o3-mini
            math.divide
               math.subtract:int:200000
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
         default
            math.divide
               math.subtract:int:2049
                  get-value:x:@.model/*/max_tokens
               .:int:2
            set-value:x:@.model/*/max_context_tokens
               get-value:x:@math.divide
            set-value:x:@.model/*/max_request_tokens
               get-value:x:@math.divide
