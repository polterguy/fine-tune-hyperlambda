
/*
 * // This Hyperlambda code calculates the maximum number of tokens for a model's context and request by dividing the difference between a model's maximum tokens and a static value (2) by 2. It then sets this calculated value as both the maximum context tokens and maximum request tokens for the model.
 * // 1. [math.divide] - Divides the result of the subtraction by 2.
 * // 2. [math.subtract] - Subtracts 2 from the model's maximum tokens.
 * // 3. [get-value:x:@.model/*/max_tokens] - Retrieves the maximum tokens value from the model.
 * // 4. [set-value:x:@.model/*/max_context_tokens] - Sets the divided value as the maximum context tokens.
 * // 5. [get-value:x:@math.divide] - Retrieves the result of the division.
 * // 6. [set-value:x:@.model/*/max_request_tokens] - Sets the divided value as the maximum request tokens.
 */
case:gpt-4-32k-0314
   math.divide
      math.subtract:int:32768
         get-value:x:@.model/*/max_tokens
      .:int:2
   set-value:x:@.model/*/max_context_tokens
      get-value:x:@math.divide
   set-value:x:@.model/*/max_request_tokens
      get-value:x:@math.divide
