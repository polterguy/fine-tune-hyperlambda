
/*
 * // This Hyperlambda code defines a dynamic slot named "magic.ai.load-robots" which is responsible for loading and parsing the robots.txt file from a specified URL. It validates the URL, retrieves the robots.txt file, and processes its content to extract directives such as "Crawl-Delay", "User-Agent", "Sitemap", "Disallow", and "Allow". The slot also handles feedback through a feedback channel and returns the extracted information or error messages if any issues occur during the retrieval process. The code ensures that the robots.txt file is fetched and parsed correctly, and the relevant data is returned for further processing.
 * // 1. [slots.create] - Creates a dynamic slot named "magic.ai.load-robots".
 * // 2. [validators.mandatory] - Ensures the "url" argument is provided.
 * // 3. [validators.url] - Validates that the "url" argument is a valid URL.
 * // 4. [if] - Checks if "headers" argument is missing and adds default headers if necessary.
 * // 5. [validators.default] - Sets default HTTP headers for the request.
 * // 6. [.domain] - Stores the domain extracted from the URL.
 * // 7. [strings.split] - Splits the URL to extract the domain.
 * // 8. [set-value] - Sets the domain value.
 * // 9. [.sitemap] - Initializes a node to store sitemap URLs.
 * // 10. [.robots] - Initializes a node to store the robots.txt URL.
 * // 11. [add] - Adds the HTTP GET request to retrieve robots.txt.
 * // 12. [strings.concat] - Constructs a message indicating the attempt to retrieve robots.txt.
 * // 13. [sockets.signal] - Sends a feedback message through the feedback channel.
 * // 14. [try] - Attempts to retrieve and process the robots.txt file.
 * // 15. [http.get] - Performs the HTTP GET request to fetch robots.txt.
 * // 16. [.disallow] - Initializes a node to store disallowed paths.
 * // 17. [.allow] - Initializes a node to store allowed paths.
 * // 18. [.crawl-delay] - Initializes a node to store crawl delay value.
 * // 19. [.has-robots] - Initializes a boolean node to indicate if robots.txt was found.
 * // 20. [if] - Checks if the HTTP response is successful and content type is text/plain.
 * // 21. [strings.split] - Splits the robots.txt content into lines.
 * // 22. [.relevant] - Initializes a boolean node to track relevance of directives.
 * // 23. [.has-seen-ainiro] - Initializes a boolean node to track if AINIRO user-agent is seen.
 * // 24. [for-each] - Iterates over each line in robots.txt.
 * // 25. [strings.trim] - Trims whitespace from each line.
 * // 26. [if] - Checks if the line is not a comment.
 * // 27. [strings.split] - Splits the line into directive and value.
 * // 28. [if] - Checks if the line has at least two parts.
 * // 29. [.value] - Stores the directive value.
 * // 30. [set-value] - Sets the trimmed directive and value.
 * // 31. [switch] - Switches based on the directive type.
 * // 32. [case] - Handles "Crawl-Delay" directive.
 * // 33. [strings.replace-not-of] - Removes non-numeric characters from crawl delay value.
 * // 34. [if] - Checks if crawl delay is valid and relevant.
 * // 35. [try] - Attempts to set crawl delay value.
 * // 36. [math.multiply] - Converts crawl delay to milliseconds.
 * // 37. [sockets.signal] - Sends a warning if crawl delay is too high.
 * // 38. [.catch] - Handles invalid crawl delay values.
 * // 39. [case] - Handles "User-Agent" directive.
 * // 40. [switch] - Switches based on user-agent value.
 * // 41. [if] - Checks if user-agent is AINIRO.
 * // 42. [remove-nodes] - Clears allow and disallow lists if AINIRO is seen.
 * // 43. [set-value] - Sets relevance based on user-agent.
 * // 44. [case] - Handles "Sitemap" directive.
 * // 45. [if] - Checks if sitemap URL is absolute.
 * // 46. [add] - Adds sitemap URL to sitemap list.
 * // 47. [else] - Constructs full sitemap URL if relative.
 * // 48. [case] - Handles "Disallow" directive.
 * // 49. [if] - Checks if disallow directive is relevant.
 * // 50. [add] - Adds disallow path to disallow list.
 * // 51. [case] - Handles "Allow" directive.
 * // 52. [if] - Checks if allow directive is relevant.
 * // 53. [add] - Adds allow path to allow list.
 * // 54. [if] - Checks if disallow list is not empty.
 * // 55. [add] - Adds disallow list to return value.
 * // 56. [if] - Checks if allow list is not empty.
 * // 57. [add] - Adds allow list to return value.
 * // 58. [if] - Checks if sitemap list is not empty.
 * // 59. [sort] - Sorts sitemap URLs by length.
 * // 60. [add] - Adds sorted sitemap list to return value.
 * // 61. [else] - Constructs default sitemap URL if none found.
 * // 62. [if] - Checks if crawl delay is not null.
 * // 63. [add] - Adds crawl delay to return value.
 * // 64. [unwrap] - Unwraps and returns the result.
 * // 65. [.catch] - Handles errors during robots.txt retrieval.
 * // 66. [sockets.signal] - Sends a warning message if error occurs.
 * // 67. [add] - Adds default sitemap URL to return value if error occurs.
 */
slots.create:magic.ai.load-robots
   validators.mandatory:x:@.arguments/*/url
   validators.url:x:@.arguments/*/url
   if
      not-exists:x:@.arguments/*/headers
      .lambda
         add:x:@.arguments
            .
               headers
   validators.default:x:@.arguments/*/headers
      User-Agent:AINIRO-Crawler 2.0
      Accept-Encoding:identity
      Accept:text/plain
   .domain
   strings.split:x:@.arguments/*/url
      .:/
   set-value:x:@.domain
      strings.concat
         get-value:x:@strings.split/0
         .://
         get-value:x:@strings.split/1
   .sitemap
   .robots
   set-value:x:@.robots
      strings.concat
         get-value:x:@.domain
         .:/robots.txt
   add:x:./*/http.get
      get-nodes:x:@.arguments/*/headers
   strings.concat
      .:"Trying to retrieve robots.txt file from "
      get-value:x:@.robots
   unwrap:x:+/**
   sockets.signal:x:@.arguments/*/feedback-channel
      args
         message:x:@strings.concat
         type:info
   try
      http.get:x:@.robots
         timeout:60
      .disallow
      .allow
      .crawl-delay
      .has-robots:bool:false
      if
         and
            mte:x:@http.get
               .:int:200
            lt:x:@http.get
               .:int:300
         .lambda
            if
               or
                  strings.starts-with:x:@http.get/*/headers/*/Content-Type
                     .:text/plain
                  strings.starts-with:x:@http.get/*/headers/*/content-type
                     .:text/plain
               .lambda
                  set-value:x:@.has-robots
                     .:bool:true
                  strings.split:x:@http.get/*/content
                     .:"\n"
                  .relevant:bool:false
                  .has-seen-ainiro:bool:false
                  for-each:x:@strings.split/*
                     strings.trim:x:@.dp/#
                        .:@"
	 "
                     if
                        not
                           strings.starts-with:x:@strings.trim
                              .:#
                        .lambda
                           strings.split:x:@strings.trim
                              .:":"
                           if
                              mte
                                 get-count:x:@strings.split/*
                                 .:int:2
                              .lambda
                                 .value
                                 set-value:x:@.value
                                    strings.join:x:@strings.split/*/[1,100]
                                       .:":"
                                 set-value:x:@strings.split/0
                                    strings.trim:x:@strings.split/0
                                       .:@"
	 "
                                 set-value:x:@.value
                                    strings.trim:x:@.value
                                       .:@"
	 "
                                 switch:x:@strings.split/0
                                    case:Crawl-Delay
                                    case:Crawl-delay
                                    case:crawl-delay
                                       set-value:x:@.value
                                          strings.replace-not-of:x:@.value
                                             .:0123456789
                                             .:
                                       if
                                          and
                                             eq:x:@.relevant
                                                .:bool:true
                                             neq:x:@.value
                                                .:
                                          .lambda
                                             try
                                                set-value:x:@.crawl-delay
                                                   math.multiply
                                                      .:int:1000
                                                      convert:x:@.value
                                                         type:int
                                                if
                                                   mt:x:@.crawl-delay
                                                      .:int:60000
                                                   .lambda
                                                      sockets.signal:x:@.arguments/*/feedback-channel
                                                         args
                                                            message:Crawl-Delay was more than 60 seconds, using 60 seconds as our value
                                                            type:warning
                                                      set-value:x:@.crawl-delay
                                                         .:int:60000
                                             .catch
                                                sockets.signal:x:@.arguments/*/feedback-channel
                                                   args
                                                      message:Crawl-Delay was not a valid integer value, using 10 seconds as our value
                                                      type:warning
                                                set-value:x:@.crawl-delay
                                                   .:int:10000
                                    case:User-Agent
                                    case:User-agent
                                    case:user-agent
                                       switch:x:@.value
                                          case:*
                                          case:AINIRO
                                          case:GPTBot
                                             if
                                                eq:x:@.value
                                                   .:AINIRO
                                                .lambda
                                                   remove-nodes:x:@.allow/*
                                                   remove-nodes:x:@.disallow/*
                                                   set-value:x:@.has-seen-ainiro
                                                      .:bool:true
                                                   set-value:x:@.relevant
                                                      .:bool:true
                                             else
                                                if
                                                   eq:x:@.has-seen-ainiro
                                                      .:bool:false
                                                   .lambda
                                                      set-value:x:@.relevant
                                                         .:bool:true
                                                else
                                                   set-value:x:@.relevant
                                                      .:bool:false
                                          default
                                             set-value:x:@.relevant
                                                .:bool:false
                                    case:Sitemap
                                    case:sitemap
                                       if
                                          or
                                             strings.starts-with:x:@.value
                                                .:"http://"
                                             strings.starts-with:x:@.value
                                                .:"https://"
                                          .lambda
                                             unwrap:x:+/*/*
                                             add:x:@.sitemap
                                                .
                                                   .:x:@.value
                                       else
                                          strings.concat
                                             get-value:x:@.domain
                                             .:/
                                             get-value:x:@.value
                                          strings.replace:x:@strings.concat
                                             .://
                                             .:/
                                          unwrap:x:+/*/*
                                          add:x:@.sitemap
                                             .
                                                .:x:@strings.replace
                                    case:Disallow
                                    case:disallow
                                       if
                                          eq:x:@.relevant
                                             .:bool:true
                                          .lambda
                                             if
                                                or
                                                   strings.starts-with:x:@.value
                                                      .:"http://"
                                                   strings.starts-with:x:@.value
                                                      .:"https://"
                                                .lambda
                                                   unwrap:x:+/*/*
                                                   add:x:@.disallow
                                                      .
                                                         .:x:@.value
                                             else
                                                strings.concat
                                                   get-value:x:@.domain
                                                   get-value:x:@.value
                                                unwrap:x:+/*/*
                                                add:x:@.disallow
                                                   .
                                                      .:x:@strings.concat
                                    case:Allow
                                    case:allow
                                       if
                                          eq:x:@.relevant
                                             .:bool:true
                                          .lambda
                                             if
                                                or
                                                   strings.starts-with:x:@.value
                                                      .:"http://"
                                                   strings.starts-with:x:@.value
                                                      .:"https://"
                                                .lambda
                                                   unwrap:x:+/*/*
                                                   add:x:@.allow
                                                      .
                                                         .:x:@.value
                                             else
                                                strings.concat
                                                   get-value:x:@.domain
                                                   get-value:x:@.value
                                                unwrap:x:+/*/*
                                                add:x:@.allow
                                                   .
                                                      .:x:@strings.concat
      if
         exists:x:@.disallow/*
         .lambda
            add:x:+/*/*
               get-nodes:x:@.disallow/*
            add:x:@try/*/return
               .
                  disallow
      if
         exists:x:@.allow/*
         .lambda
            add:x:+/*/*
               get-nodes:x:@.allow/*
            add:x:@try/*/return
               .
                  allow
      if
         exists:x:@.sitemap/*
         .lambda
            sort:x:@.sitemap/*
               if
                  lt
                     strings.length:x:@.lhs/#
                     strings.length:x:@.rhs/#
                  .lambda
                     set-value:x:@.result
                        .:int:-1
               else-if
                  mt
                     strings.length:x:@.lhs/#
                     strings.length:x:@.rhs/#
                  .lambda
                     set-value:x:@.result
                        .:int:1
               else
                  set-value:x:@.result
                     .:int:0
            add:x:+/*/*
               get-nodes:x:@sort/*
            add:x:@try/*/return
               .
                  sitemap
      else
         strings.concat
            get-value:x:@.domain
            .:/sitemap.xml
         unwrap:x:+/*/*/*
         add:x:@try/*/return
            .
               sitemap
                  .:x:@strings.concat
      if
         not-null:x:@.crawl-delay
         .lambda
            unwrap:x:+/*/*
            add:x:@try/*/return
               .
                  crawl-delay:x:@.crawl-delay
      unwrap:x:+/*
      return
         found:x:@.has-robots
   .catch
      strings.concat
         .:"Error while trying to retrieve robots.txt, message was; '"
         get-value:x:@.arguments/*/message
         .:"'"
      unwrap:x:+/**
      sockets.signal:x:@.arguments/@.arguments/*/feedback-channel
         args
            message:x:@strings.concat
            type:warning
      strings.concat
         get-value:x:@.domain
         .:/sitemap.xml
      unwrap:x:+/*/*
      return
         found:bool:false
         sitemap
            .:x:@strings.concat
