/*
 * Returns a list of available OpenAI models, including fine-tuned and public models,
 * with additional metadata about token limits and feature support (e.g. chat or vector embedding).
 *
 * This endpoint simplifies client-side logic by enriching the OpenAI `/v1/models` response
 * with contextual information about each model's capabilities. It also supports optional
 * use of a custom API key and caches results for 3 minutes to reduce load and latency.
 *
 * INPUT PARAMETERS:
 * - [api_key:string] (optional): A user-supplied API key used to override the system default.
 *
 * EXECUTION FLOW:
 *
 * 1. **Authorization**:
 *    - Ensures that only root users can invoke this endpoint using [auth.ticket.verify:root].
 *
 * 2. **Bearer Token Setup**:
 *    - Retrieves the OpenAI API key from either the user-supplied [api_key] or from configuration ([magic:openai:key]).
 *    - Constructs the proper Bearer token required to authenticate with OpenAI's API.
 *
 * 3. **Model Fetching**:
 *    - Sends an authenticated HTTP GET request to `https://api.openai.com/v1/models`.
 *    - Parses and converts the response into a lambda-compatible structure.
 *    - If the request fails (non-200 status), logs the issue and returns a 500 error with a generic message.
 *
 * 4. **Model Metadata Enrichment**:
 *    - Iterates through the returned list of models.
 *    - For known models (by ID), adds relevant metadata such as:
 *        - [tokens]: Maximum token context size.
 *        - [chat:bool]: Whether the model supports chat-style prompting.
 *        - [vector:bool]: Whether the model supports embedding/vectorization.
 *    - These enhancements help consumers choose the correct model for their needs.
 *
 * 5. **Caching Policy**:
 *    - Applies an HTTP cache control header to allow private caching of the result for up to 180 seconds (3 minutes).
 *    - This prevents unnecessary re-fetching of the model list for frequently accessed tools like UIs or dashboards.
 *
 * 6. **Response**:
 *    - Returns the enriched model list to the caller as a structured node set, ready for consumption by frontends or API clients.
 *
 * This endpoint is useful for UI dropdowns, automation pipelines, and dashboards that need to display
 * available OpenAI models along with their usage characteristics without hardcoding model knowledge client-side.
 */


.arguments
   api_key:string

// Making sure user has access to invoked endpoint.
auth.ticket.verify:root

/*
 * Creating our Bearer token by reading our OpenAI
 * configuration settings.
 */
.token
set-value:x:@.token
   strings.concat
      .:"Bearer "
      get-first-value
         get-value:x:@.arguments/*/api_key
         config.get:"magic:openai:key"

// Creating an HTTP POST request towards OpenAI, now decorated.
http.get:"https://api.openai.com/v1/models"
   headers
      Authorization:x:@.token
   convert:true

// Sanity checking above invocation
if
   neq:x:@http.get
      .:int:200
   .lambda

      // Oops, error - Logging error and returning status 500 to caller.
      lambda2hyper:x:@http.get
      log.error:Something went wrong while invoking OpenAI
         error:x:@lambda2hyper
      response.status.set:500
      return
         message:Something went wrong while invoking OpenAI, check your log for details

// Adding token count to models we know token count for.
for-each:x:@http.get/*/content/*/data/*

   switch:x:@.dp/#/*/id

      case:gpt-4o
      case:gpt-4o-mini
      case:gpt-4o-2024-05-13
      case:gpt-4-turbo
      case:gpt-4-0125-preview
      case:gpt-4-1106-preview
      case:gpt-4-turbo-preview
      case:gpt-4-turbo-2024-04-09
      case:gpt-4-vision-preview
      case:gpt-4-1106-vision-preview
         add:x:@.dp/#
            .
               tokens:int:128000
               chat:bool:true

      case:gpt-4
      case:gpt-4-0613
         add:x:@.dp/#
            .
               tokens:int:8192
               chat:bool:true

      case:gpt-4-32k
      case:gpt-4-32k-0613
         add:x:@.dp/#
            .
               tokens:int:32768
               chat:bool:true

      case:gpt-3.5-turbo
      case:gpt-3.5-turbo-0125
      case:gpt-3.5-turbo-1106
      case:gpt-3.5-turbo-16k
      case:gpt-3.5-turbo-16k-0613
         add:x:@.dp/#
            .
               tokens:int:16385
               chat:bool:true

      case:gpt-3.5-turbo-0613
         add:x:@.dp/#
            .
               tokens:int:4096
               chat:bool:true

      case:gpt-3.5-turbo-instruct
         add:x:@.dp/#
            .
               tokens:int:4096
               chat:bool:false

      case:text-embedding-3-small
      case:text-embedding-ada-002
         add:x:@.dp/#
            .
               vector:bool:true

      case:o3-mini
         add:x:@.dp/#
            .
               tokens:int:200000
               chat:bool:true

      case:gpt-4.5-preview
         add:x:@.dp/#
            .
               tokens:int:128000
               chat:bool:true

/*
 * Applying some HTTP caching to avoid invoking OpenAI again with
 * the same question before some minimum amount of time has passed.
 */
response.headers.set
   Cache-Control:private, max-age=180

// Returning results returned from invocation above to caller
add:x:../*/return
   get-nodes:x:@http.get/*/content/*/data/*
return
