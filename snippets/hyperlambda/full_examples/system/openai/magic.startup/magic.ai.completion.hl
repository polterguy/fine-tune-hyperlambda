
/*
 * Executes a completion-style OpenAI request using a legacy completion model (non-chat), such as `text-davinci-003`.
 *
 * This slot mimics the behavior of [magic.ai.chat] but is designed for backward compatibility with older OpenAI completion models
 * that do not support role-based conversations. It's primarily used for fine-tuned models or simpler prompt/response setups.
 *
 * ⚠️ This is a **legacy slot**, largely deprecated in favor of chat-based slots such as [magic.ai.chat], and only preserved for
 * support with older fine-tuned models and APIs.
 *
 * ARGUMENTS:
 * - [prompt:string]: The question or instruction for the model.
 * - [type:string]: The model type (used for embedding, logging, caching, etc.).
 * - [model:string]: OpenAI model ID (e.g., text-davinci-003 or a fine-tuned model).
 * - [temperature:double]: Model creativity/variance.
 * - [max_tokens:long]: Max token count for the response.
 * - [prefix:string] (optional): Optional prefix added to the prompt (can include [QUESTION], [CONTEXT], [ANSWER]).
 * - [cached:int] (optional): Enables result caching if supported.
 * - [references:bool] (optional): Whether to return RAG snippets as references.
 * - [use_embeddings:int] (optional): Enables retrieval-augmented generation from a vector store.
 * - [threshold:decimal], [max_context_tokens:long]: Used for filtering relevant snippets.
 * - [session:string], [user_id:string], [referrer:string]: Metadata used for session tracking and supervised logging.
 * - [supervised:int]: If true (1), stores conversation in [ml_requests].
 * - [api_key:string] (optional): Overrides the OpenAI API key for the request.
 *
 * EXECUTION FLOW:
 *
 * 1. **Cache Check (if enabled)**:
 *    - If caching is enabled and the user hasn't requested references, the slot checks for an existing response
 *      for the same [prompt] and [type]. If a cached match is found, it is returned immediately.
 *
 * 2. **Embedding/RAG Context Retrieval**:
 *    - If [use_embeddings] is enabled, the slot fetches relevant snippets via [magic.ai.get-context].
 *    - The context is injected into the prompt using either a structured [prefix] (with placeholders) or a default format.
 *    - If [references:true] is set, it returns the associated context snippets in the response.
 *
 * 3. **Prompt Preparation (Non-RAG Models)**:
 *    - If the model name contains a colon (`:`), the system assumes it is fine-tuned and adds `->` and `END` stop patterns.
 *    - Otherwise, the raw [prompt] is sent as-is.
 *
 * 4. **OpenAI Completion Invocation**:
 *    - Calls the OpenAI Completion API (`/v1/completions`) with the constructed [prompt], [model], and other settings.
 *    - Handles errors gracefully by logging and returning a structured error response.
 *
 * 5. **Post-Processing**:
 *    - Trims the returned completion text.
 *    - Adds `finish_reason` and (if applicable) `references` and `db_time` to the response payload.
 *
 * 6. **Session Logging (if supervised)**:
 *    - If [supervised:1] and no error occurred, stores the full exchange (prompt, completion, metadata) into [ml_requests].
 *
 * 7. **Response Caching Headers**:
 *    - Sets `Cache-Control: max-age=30` to reduce redundant completions for identical requests.
 *
 * 8. **Return**:
 *    - Returns a clean structure with:
 *        - [result:string]: The AI's response.
 *        - [finish_reason:string]: OpenAI's signal for why it stopped.
 *        - [references:*] (optional): Snippets used for RAG (if applicable).
 *        - [db_time:string] (optional): Duration of vector DB lookup.
 *
 * USAGE SCENARIOS:
 * - Legacy integrations using fine-tuned completion models.
 * - Non-chat-based Q&A systems where prompt injection is controlled externally.
 * - Situations requiring caching or lightweight RAG support for non-conversational models.
 *
 * DEPRECATION NOTICE:
 * - Use [magic.ai.chat] for all GPT-style or chat-capable models (gpt-4, gpt-3.5, etc.).
 * - This slot is retained for backward compatibility only.
 */

slots.create:magic.ai.completion

   /*
    * Checking if model is configured to return cached requests.
    *
    * Notice, the type needs to have turned on cache, and we need to have a
    * historical request that's cached matching the prompt and the type,
    * and caller must not have asked for references for the cache to kick in.
    */
   if
      and
         exists:x:@.arguments/*/cached
         not-null:x:@.arguments/*/cached
         eq
            convert:x:@.arguments/*/cached
               type:int
            .:int:1
         or
            not-exists:x:@.arguments/*/references
            not
               get-value:x:@.arguments/*/references
      .lambda

         // Checking if we can find a cached request matching prompt and type.
         data.connect:[generic|magic]
            data.read
               table:ml_requests
               columns
                  completion
                  cached
               where
                  and
                     prompt.eq:x:@.arguments/*/prompt
                     type.eq:x:@.arguments/*/type
               limit:1
               order:created
               direction:desc

            // Checking if we could find a matching request in cache.
            if
               and
                  exists:x:@data.read/*/*
                  eq
                     convert:x:@data.read/*/*/cached
                        type:int
                     .:int:1
               .lambda

                  // Returning cached completion.
                  unwrap:x:+/*
                  return
                     result:x:@data.read/*/*/completion
                     finish_reason:cached

   // Result we're returning to caller.
   .result

   // Prompt that might be changed if model contains a prefix, and/or we're using embeddings.
   .prompt

   /*
    * Checking if model is configured to use embeddings.
    *
    * If model is configured to using embeddings, we find relevant training snippets from
    * training data and use as "CONTEXT". If not, it might be either a fine-tuned model,
    * or a "mint" model, where mint implies no context or customisation having been done,
    * which is true if user just wants the default AI model without any customisations.
    */
   if
      eq
         convert:x:@.arguments/*/use_embeddings
            type:int
         .:int:1
      .lambda

         // Retrieving relevant snippets.
         unwrap:x:+/*
         signal:magic.ai.get-context
            type:x:@.arguments/*/type
            prompt:x:@.arguments/*/prompt
            threshold:x:@.arguments/*/threshold
            max_tokens:x:@.arguments/*/max_context_tokens
            api_key:x:@.arguments/*/api_key

         // Checking if we've got a cached result.
         if
            exists:x:@signal/*/cached
            .lambda

               // Cached result.
               unwrap:x:+/*
               return
                  result:x:@signal/*/cached
                  finish_reason:cached

         // Adding db_time to [.result].
         add:x:@.result
            get-nodes:x:@signal/*/db_time

         // Correctly applying prefix.
         if
            and
               strings.contains:x:@.arguments/*/prefix
                  .:[CONTEXT]
               strings.contains:x:@.arguments/*/prefix
                  .:[QUESTION]
               strings.contains:x:@.arguments/*/prefix
                  .:"ANSWER:"
            .lambda

               // Structured prefix.
               set-value:x:@.prompt
                  strings.replace:x:@.arguments/*/prefix
                     .:[QUESTION]
                     get-value:x:@.arguments/*/prompt
               set-value:x:@.prompt
                  strings.replace:x:@.prompt
                     .:[CONTEXT]
                     get-value:x:@signal/*/context

         else

            // Old style prefix.
            set-value:x:@.prompt
               strings.concat
                  get-value:x:@.arguments/*/prefix
                  .:"\r\nQUESTION: "
                  get-value:x:@.arguments/*/prompt
                  .:"\r\nCONTEXT: \r\n"
                  get-value:x:@signal/*/context
                  .:"\r\nANSWER: "

         // Checking if caller wants references.
         if
            and
               exists:x:@.arguments/*/references
               not-null:x:@.arguments/*/references
               get-value:x:@.arguments/*/references
            .lambda
               add:x:@.result
                  .
                     references
               add:x:@.result/*/references
                  get-nodes:x:@signal/*/snippets/*

   else

      /*
       * Not using embeddings, either a mint model or a fine-tuned model.
       *
       * Checking if model is fine-tuned, which is true if model name contains
       * a colon (:), at which point we apply "->" and " END" to help model roundtrip.
       */
      if
         strings.contains:x:@.arguments/*/model
            .:":"
         .lambda

            // Fine-tuned model, making sure we apply STOP characters to prompt and invocation.
            set-value:x:@.prompt
               strings.concat
                  get-value:x:@.arguments/*/prompt
                  .:" ->"
            add:x:../*/http.post/*/payload
               .
                  stop:" END"

      else

         // "Mint" model - Implying default model without any customisations.
         set-value:x:@.prompt
            get-value:x:@.arguments/*/prompt

   // Retrieving token.
   .token
   set-value:x:@.token
      strings.concat
         .:"Bearer "
         get-first-value
            get-value:x:@.arguments/*/api_key
            config.get:"magic:openai:key"

   // Invoking OpenAI, now with a decorated prompt.
   http.post:"https://api.openai.com/v1/completions"
      headers
         Authorization:x:@.token
         Content-Type:application/json
      payload
         prompt:x:@.prompt
         model:x:@.arguments/*/model
         max_tokens:x:@.arguments/*/max_tokens
         temperature:x:@.arguments/*/temperature
      convert:true

   // Sanity checking above invocation.
   if
      not
         and
            mte:x:@http.post
               .:int:200
            lt:x:@http.post
               .:int:300
      .lambda

         // Oops, error - Logging error and returning status 500 to caller.
         lambda2hyper:x:@http.post
         log.error:Something went wrong while invoking OpenAI
            message:x:@http.post/*/content/*/error/*/message
            status:x:@http.post
            error:x:@lambda2hyper
         response.status.set:x:@http.post
         unwrap:x:+/*
         return
            error:bool:true
            result:x:@http.post/*/content/*/error/*/message
   else

      // Success! Logging as such!
      log.info:Invoking OpenAI was a success

   // Making sure we trim response before adding it to [.result].
   strings.trim:x:@http.post/*/content/*/choices/0/*/text
   unwrap:x:+/*/*
   add:x:@.result
      .
         result:x:@strings.trim

   // Making sure we return finish reason to caller.
   get-first-value
      get-value:x:@http.post/*/content/*/choices/0/*/finish_reason
      .:unknown
   unwrap:x:+/*/*
   add:x:@.result
      .
         finish_reason:x:@get-first-value

   // Re-opening database connection since it might have timed out while we're waiting for OpenAI.
   data.connect:[generic|magic]

      // Checking if type is 'supervised', at which point we store prompt and completion.
      if
         and
            not
               get-value:x:@.result/*/error
            not
               exists:x:@data.read/*/*
            mt
               convert:x:@.arguments/*/supervised
                  type:int
               .:int:0
         .lambda

            // Storing prompt and completion in ml_requests table.
            data.create
               table:ml_requests
               values
                  type:x:@.arguments/*/type
                  prompt:x:@.arguments/*/prompt
                  completion:x:@.result/*/result
                  finish_reason:x:@.result/*/finish_reason
                  session:x:@.arguments/*/session
                  user_id:x:@.arguments/*/user_id
                  referrer:x:@.arguments/*/referrer

   /*
    * Applying some HTTP caching to avoid invoking OpenAI again with
    * the same question before some minimum amount of time has passed.
    */
   response.headers.set
      Cache-Control:max-age=30

   // Returning results returned from invocation above to caller.
   add:x:+
      get-nodes:x:@.result/*/result
      get-nodes:x:@.result/*/finish_reason
      get-nodes:x:@.result/*/references
      get-nodes:x:@.result/*/db_time
   return
