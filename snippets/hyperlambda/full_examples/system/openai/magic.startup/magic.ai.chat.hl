
/*
 * Create a chat slot for OpenAI GPT models with full context management and optional streaming.
 *
 * This dynamic slot handles full conversational logic with OpenAI’s GPT models.
 * It integrates database-driven retrieval-augmented generation (RAG), caching, system message setup,
 * session memory, and support for multi-turn conversations. It also handles OpenAI function calling
 * with dynamic slot execution and SSE-based token streaming to clients.
 *
 * ---
 * ### Features:
 *
 * - **RAG Integration**: Retrieves contextual snippets for grounding the conversation.
 * - **Cache Checking**: Returns cached completions when appropriate.
 * - **Session Memory**: Uses previous messages and context from `ml_requests` table.
 * - **Prompt Management**: Includes support for prefixing, context trimming, and system message injection.
 * - **Streaming Support**: Allows real-time SSE token streaming to WebSocket clients.
 * - **Function Calling**: Parses `FUNCTION_INVOCATION[...]` markers and dynamically executes related slots.
 * - **Token Budget Management**: Prunes conversation to fit the model’s token window.
 * - **Integration Hooks**: Supports webhook-based outgoing message integrations.
 *
 * ---
 * ### Key Arguments:
 *
 * - `type` (string): ML type for context and categorization.
 * - `prompt` (string): The question or message to send.
 * - `model` (string): OpenAI model to use (`gpt-4`, `gpt-3.5-turbo`, `o3-mini`, etc).
 * - `max_tokens` (int): Max tokens for OpenAI response.
 * - `system_message` (string, optional): Prepended instruction to guide the assistant.
 * - `session` (string, optional): Enables session memory via caching.
 * - `stream` (bool): Enables token streaming via SSE.
 * - `user_id`, `referrer` (string): Metadata for storage and tracing.
 * - `supervised` (bool): If true, stores conversation to `ml_requests` database.
 * - `max_function_invocations` (int): Max recursive OpenAI+function iterations.
 *
 * ---
 * ### Output:
 *
 * - If `stream=true`, returns nothing directly; tokens stream via sockets.
 * - Otherwise, returns:
 *   - `result`: Final response from OpenAI
 *   - `finish_reason`: Why OpenAI stopped
 *   - `db_time`: Optional database latency
 *   - `references`: RAG snippet references if enabled
 *
 * ---
 * ### Example Use:
 *
 * ```json
 * {
 *   "type": "support",
 *   "prompt": "How can I reset my password?",
 *   "model": "gpt-4",
 *   "stream": false,
 *   "session": "chat-abc123",
 *   "supervised": true,
 *   "max_function_invocations": 2
 * }
 * ```
 *
 * ---
 * This slot serves as the core backend for any GPT-powered assistant using Magic Cloudlet,
 * supporting both supervised fine-tuning data collection and real-time chat UX.
 */
slots.create:magic.ai.chat

   // Applying defaults.
   validators.default:x:@.arguments
      stream:bool:false

   // Retrieving relevant snippets.
   unwrap:x:+/*
   signal:magic.ai.get-context
      type:x:@.arguments/*/type
      prompt:x:@.arguments/*/prompt
      threshold:x:@.arguments/*/threshold
      max_tokens:x:@.arguments/*/max_context_tokens
      api_key:x:@.arguments/*/api_key
      search_postfix:x:@.arguments/*/search_postfix
      session:x:@.arguments/*/session
      session_timeout:x:@.arguments/*/session_timeout

   // Checking if we've got a cached result.
   if
      exists:x:@signal/*/cached
      .lambda

         // Cached result, checking if type is "supervised".
         if
            and
               exists:x:@.arguments/*/supervised
               not-null:x:@.arguments/*/supervised
               eq
                  convert:x:@.arguments/*/supervised
                     type:int
                  .:int:1
            .lambda

               // Storing response to ml_requests to keep history.
               data.connect:[generic|magic]
                  data.create
                     table:ml_requests
                     values
                        type:x:@.arguments/*/type
                        prompt:x:@.arguments/*/prompt
                        completion:x:@signal/*/cached
                        finish_reason:cached
                        session:x:@.arguments/*/session
                        user_id:x:@.arguments/*/user_id
                        referrer:x:@.arguments/*/referrer

         // Returning cached result to caller.
         unwrap:x:+/*
         return
            result:x:@signal/*/cached
            finish_reason:cached
            stream:bool:false

   /*
    * Session for user containing previous questions and answers,
    * in addition to context and system message.
    */
   .session

   // Checking if we've got a system message.
   if
      and
         not-null:x:@.arguments/*/system_message
         neq:x:@.arguments/*/system_message
            .:
      .lambda

         // Adding system message to [.session]
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:system
                  content:x:@.arguments/*/system_message

   // Then adding context ml_requests belonging to user to [.session]
   data.connect:[generic|magic]

      // Selecting context requests from database.
      data.select:@"
select prompt, max(completion) as completion
   from ml_requests
   where user_id = @user_id and questionnaire = 1 and context = 1
   group by prompt"
         user_id:x:@.arguments/*/user_id

      // Adding context requests to [.session].
      for-each:x:@data.select/*

         // Adding question as system message.
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:assistant
                  content:x:@.dp/#/*/prompt

         // Adding answer as user message.
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:user
                  content:x:@.dp/#/*/completion

   /*
    * Storing how many fixed messages we have to avoid removing
    * system message or questionnaire messages as we're pruning messages
    * to avoid overflowing model.
    */
   .fixed
   set-value:x:@.fixed
      get-count:x:@.session/*

   // Checking if we've got a cached session.
   if
      and
         exists:x:@.arguments/*/session
         not-null:x:@.arguments/*/session
         neq:x:@.arguments/*/session
            .:
      .lambda

         // Caller provided a cache key.
         cache.get:x:@.arguments/*/session
         if
            not-null:x:@cache.get
            .lambda

               /*
                * Adding session questions to context, making sure we never use more than
                * a maximum of 'max_session_items' taken from ml_types. Since each question contains a
                * context, a question, and an answer, this implies we're never keep more than
                * maximum 'max_session_items' items in our session.
                */
               convert:x:@.arguments/*/max_session_items
                  type:int
               hyper2lambda:x:@cache.get
               while
                  mt
                     get-count:x:@hyper2lambda/*
                     get-value:x:@convert
                  .lambda
                     remove-nodes:x:@hyper2lambda/0
               add:x:@.session
                  get-nodes:x:@hyper2lambda/*

   // Checking if we've got context.
   if
      not-null:x:@signal/*/context
      .lambda

         // Adding context to above [.session].
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:system
                  content:x:@signal/*/context

   // Checking if we've got a [data] argument, and if so, making sure we accommodate for token size.
   .extra-tokens:int:0
   if
      and
         exists:x:@.arguments/*/data
         not-null:x:@.arguments/*/data
         neq:x:@.arguments/*/data
            .:
      .lambda
         set-value:x:@.extra-tokens
            openai.tokenize:x:@.arguments/*/data

   // Checking if model is prefixed, at which point we prepend prefix to question.
   .only-prompt
   set-value:x:@.only-prompt
      get-value:x:@.arguments/*/prompt
   if
      and
         exists:x:@.arguments/*/prefix
         not-null:x:@.arguments/*/prefix
         neq:x:@.arguments/*/prefix
            .:
      .lambda

         // Prefixing prompt.
         set-value:x:@.arguments/*/prompt
            strings.concat
               get-value:x:@.arguments/*/prefix
               get-value:x:@.arguments/*/prompt

   // Checking if we've got a [data] argument, and if so, adding it.
   if
      and
         exists:x:@.arguments/*/data
         not-null:x:@.arguments/*/data
         neq:x:@.arguments/*/data
            .:
      .lambda
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:user
                  content:x:@.arguments/*/data

   // Adding user's current question to session.
   unwrap:x:+/*/*/*/content
   add:x:@.session
      .
         .
            role:user
            content:x:@.arguments/*/prompt

   // Checking if this is an o3 model.
   if
      eq:x:@.arguments/*/model
         .:o3-mini
      .lambda

         // Modifying messages to make sure we conform to o3 structure.
         set-value:x:@.session/**/role/=system
            .:developer
         set-value:x:@.session/**/role/=developer/[0,1]/./*/content
            strings.concat:x:@.session/**/role/=developer/[0,1]/./*/content
               .:"Formatting re-enabled\n\n"
               get-value:x:@.session/**/role/=developer/[0,1]/./*/content

   /*
    * Pruning previous messages until size of context + max_tokens is less than whatever
    * amount of tokens the model can handle.
    */
   .cont:bool:true
   while:x:@.cont

      lambda2hyper:x:@.session/*
      if
         lt
            math.add
               get-value:x:@.arguments/*/max_tokens
               openai.tokenize:x:@lambda2hyper
               get-value:x:@.extra-tokens
            get-value:x:@.arguments/*/model_size
         .lambda

            // Context is small enough to answer according to max_tokens.
            set-value:x:@.cont
               .:bool:false

      else

         /*
          * We need to remove one of our previous messages to have OpenAI return max_tokens.
          * Notice, we only remove messages after our "fixed" messages, which are system message and
          * questionnaire messages.
          */
         strings.concat
            .:@.session/
            get-value:x:@.fixed
         set-x:x:./*/remove-nodes
            convert:x:@strings.concat
               type:x
         remove-nodes

   // Retrieving token used to invoke OpenAI.
   .token
   set-value:x:@.token
      strings.concat
         .:"Bearer "
         get-first-value
            get-value:x:@.arguments/*/api_key
            config.get:"magic:openai:key"

   /*
    * Checking if caller wants to stream tokens.
    *
    * Streaming implies using OpenAI's SSE features, where the
    * invocation returns immediately, for then to invoke our callback
    * as more tokens are available to our client.
    */
   if
      eq:x:@.arguments/*/stream
         .:bool:true
      .lambda

         // We're using streaming or SSE as we're invoking OpenAI.
         unwrap:x:+/+/+/+/*/*/.sse/*/.extra
         unwrap:x:+/+/+/*/*/.sse/*/.user-id
         unwrap:x:+/+/*/*/.sse/*/.type
         unwrap:x:+/*/*/.sse/*/.session
         add:x:../*/.invoke/*/while/*/.lambda/*/http.post
            .
               .sse

                  /*
                   * Forward evaluated further up, contains the session ID,
                   * and is also our socket channel for notifying client
                   * about new tokens received from OpenAI.
                   */
                  .session:x:@.arguments/*/session

                  // Extra information from client.
                  .extra:x:@.arguments/*/extra

                  // Type forward declared further up in file.
                  .type:x:@.arguments/*/type

                  // Type forward declared further up in file.
                  .user-id:x:@.arguments/*/user_id

                  // Verifying OpenAI sent us actual content.
                  if
                     and
                        exists:x:@.arguments/*/message
                        not-null:x:@.arguments/*/message
                        strings.starts-with:x:@.arguments/*/message
                           .:"data:"
                     .lambda

                        // Parsing message sent to us from OpenAI.
                        strings.substring:x:@.arguments/*/message
                           .:int:5
                        strings.trim:x:@strings.substring

                        // Making sure OpenAI sent us something at all.
                        if
                           neq:x:@strings.trim
                              .:
                           .lambda

                              // Checking if we're done.
                              if
                                 eq:x:@strings.trim
                                    .:[DONE]
                                 .lambda

                                    // Checking if we're counting invocations.
                                    if
                                       neq:x:@.no-invocation
                                          .:int:0
                                       .lambda

                                          /*
                                           * Incrementing invocation count since we've already started counting
                                           * due to function invocations.
                                           */
                                          math.increment:x:@.no-invocation

                                    // Checking if we've got function invocations.
                                    if
                                       and
                                          strings.contains:x:@.result
                                             .:"___"
                                          strings.contains:x:@.result
                                             .:"FUNCTION_INVOCATION["
                                       .lambda

                                          /*
                                           * We've got function invocations in response from OpenAI.
                                           *
                                           * Invoking functions and invoking ChatGPT afterwards with response from function invocations.
                                           */
                                          if
                                             eq:x:@.no-invocation
                                                .:int:0
                                             .lambda

                                                // Incrementing our invocation count initially.
                                                math.increment:x:@.no-invocation

                                          // Iterating through all function invocations, in case there are more than one.
                                          strings.split:x:@.result
                                             .:"___"
                                          for-each:x:@strings.split/*

                                             /*
                                              * Sometimes OpenAI returns additional information after a function invocation,
                                              * even though we're explicitly telling it not to. Therefore we need to verify
                                              * that this actually is a function invocation before attempting to invoke it.
                                              */
                                             if
                                                strings.contains:x:@.dp/#
                                                   .:FUNCTION_INVOCATION[/
                                                .lambda

                                                   // Executing function such that we can catch exceptions and provide meaningful feedback.
                                                   try

                                                      // Checking we're allowed to invoke functions another time.
                                                      if
                                                         lte:x:@.iterations
                                                            .:int:1
                                                         .lambda

                                                            // Too many function invocations
                                                            log.error:Too many function invocations
                                                            sockets.signal:x:@.session
                                                               args
                                                                  function_error:Too many function invocations
                                                            sockets.signal:x:@.session
                                                               args
                                                                  function_waiting:bool:false
                                                            sockets.signal:x:@.session
                                                               args
                                                                  error:bool:true
                                                                  status:int:500
                                                                  message:Too many function invocations. Configure your type to handle more invocations or change your prompt.
                                                            sockets.signal:x:@.session
                                                               args
                                                                  finished:bool:true
                                                      else

                                                         // Executing Hyperlambda function file.
                                                         execute:magic.ai.functions.invoke
                                                            type:x:@.type
                                                            session:x:@.session
                                                            user-id:x:@.user-id
                                                            invocation:x:@.dp/#
                                                            extra:x:@.extra

                                                         // Transforming result to JSON.
                                                         lambda2json:x:@execute/*/result/*
                                                            format:true

                                                         // Verifying result of invocation doesn't overflow maximum context tokens.
                                                         if
                                                            mt
                                                               openai.tokenize:x:@lambda2json
                                                               get-value:x:@.arguments/@.arguments/*/max_context_tokens
                                                            .lambda

                                                               // Oops, invocation is overflowing context window.
                                                               throw:Result too large, try to limit your result
                                                                  status:int:400
                                                                  public:bool:true

                                                         // Function file succeeded, returning result to caller.
                                                         config.get:"magic:chat:functions:success-message"
                                                            .:"Success!"
                                                         unwrap:x:+/*/*
                                                         sockets.signal:x:@.session
                                                            args
                                                               function_result:x:@config.get
                                                               invocation:x:@execute/*/json
                                                               file:x:@execute/*/workflow

                                                         // Adding result of function invocation to [.new-prompt] buffer.
                                                         if
                                                            exists:x:@execute/*/result/*
                                                            .lambda

                                                               /*
                                                                * We have a response from our function invocation and therefore we can
                                                                * safely transform lambda to JSON for our next prompt.
                                                                */
                                                               set-value:x:@.new-prompt
                                                                  strings.concat
                                                                     get-value:x:@.new-prompt
                                                                     .:"Response from '"
                                                                     get-value:x:@execute/*/workflow
                                                                     .:"' was:\r\n```json\r\n"
                                                                     get-value:x:@lambda2json
                                                                     .:"\r\n"
                                                                     .:"```"
                                                                     .:"\r\n"
                                                                     .:"\r\n"

                                                               // Storing result of function invocation such that we can store it in ml_requests.
                                                               set-value:x:@.function-result
                                                                  strings.concat
                                                                     get-value:x:@.function-result
                                                                     get-value:x:@lambda2json
                                                                     .:"\n"

                                                         else

                                                            /*
                                                             * We have no respone value from invocation but we still make sure our next prompt
                                                             * knows it was a success invoking function.
                                                             */
                                                            set-value:x:@.new-prompt
                                                               strings.concat
                                                                  get-value:x:@.new-prompt
                                                                  .:"Invocation of '"
                                                                  get-value:x:@execute/*/workflow
                                                                  .:"' was a success."
                                                                  .:"\r\n"
                                                                  .:"\r\n"

                                                   .catch

                                                      // Function file failed.
                                                      log.error:Could not execute AI function
                                                         message:x:@.arguments/*/message
                                                      unwrap:x:+/*/*
                                                      sockets.signal:x:@.session
                                                         args
                                                            function_error:x:@.arguments/*/message

                                                      // Updating buffer for prompt with failure information.
                                                      set-value:x:@.new-prompt
                                                         strings.concat
                                                            get-value:x:@.new-prompt
                                                            .:"Invocation failed, exception message was: '"
                                                            get-value:x:@.arguments/*/message
                                                            .:"'"
                                                            .:"\r\n"
                                                            .:"\r\n"

                                          // Trimming new prompt.
                                          set-value:x:@.new-prompt
                                             strings.trim:x:@.new-prompt

                                    else

                                       // OpenAI is done.
                                       sockets.signal:x:@.session
                                          args
                                             finished:bool:true
                                       set-value:x:@.iterations
                                          .:int:0

                              else

                                 // More data to come.
                                 json2lambda:x:@strings.trim

                                 // Updating finish reason if we were given a reason.
                                 if
                                    and
                                       exists:x:@json2lambda/*/choices/0/*/finish_reason
                                       not-null:x:@json2lambda/*/choices/0/*/finish_reason
                                    .lambda

                                       // OpenAI returned a [finish_reason].
                                       set-value:x:@.finish_reason
                                          get-value:x:@json2lambda/*/choices/0/*/finish_reason

                                       // Signaling client
                                       unwrap:x:+/*/*
                                       sockets.signal:x:@.session
                                          args
                                             finish_reason:x:@.finish_reason

                                 // Making sure we have a message.
                                 if
                                    and
                                       not-null:x:@json2lambda/*/choices/0/*/delta/*/content
                                       neq:x:@json2lambda/*/choices/0/*/delta/*/content
                                          .:
                                    .lambda

                                       // Appending content to above [.result].
                                       set-value:x:@.result
                                          strings.concat
                                             get-value:x:@.result
                                             get-value:x:@json2lambda/*/choices/0/*/delta/*/content

                                       /*
                                        * Making sure we do NOT send FUNCTION_INVOCATION message parts to client.
                                        *
                                        * Notice, FUNCTION_INVOCATION parts are always supposed to be added at the
                                        * end of the response, so this implies if we've got a function invocation, there
                                        * should be no more relevant data after the function invocation.
                                        */
                                       if
                                          not
                                             strings.contains:x:@.result
                                                .:"___"
                                          .lambda

                                             // Signaling client with the newly acquired data from OpenAI.
                                             unwrap:x:+/*/*
                                             sockets.signal:x:@.session
                                                args
                                                   message:x:@json2lambda/*/choices/0/*/delta/*/content

                                       else-if
                                          and
                                             strings.contains:x:@.result
                                                .:"FUNCTION_INVOCATION["
                                             neq:x:@.has-sent-waiting
                                                .:bool:true
                                          .lambda

                                             // Signaling frontend that we're waiting for function invocation to respond.
                                             sockets.signal:x:@.session
                                                args
                                                   function_waiting:bool:true
                                             set-value:x:@.has-sent-waiting
                                                .:bool:true

         /*
          * Making sure we parametrise invocation to OpenAI
          * such that it streams result.
          */
         add:x:../*/.invoke/*/while/*/.lambda/*/http.post/*/payload
            .
               stream:bool:true

   /*
    * Lambda object actually invoking OpenAI.
    *
    * Notice, if we're streaming the result, this will be invoked one a different thread,
    * otherwise it will be invoked synchronously.
    */
   insert-before:x:./*/.invoke/0
      get-nodes:x:@.arguments
      get-nodes:x:@.session
      get-nodes:x:@.token
      get-nodes:x:@.fixed
      get-nodes:x:@.only-prompt
   .invoke

      /*
       * As long as this is more than 0, we will continue invoking OpenAI,
       * since OpenAI might return one or more function invocations that requires further
       * analysis in a loop.
       *
       * If OpenAI does not return function invocations, we set this to 0, which stops
       * our loop invoking OpenAI
       */
      .iterations
      set-value:x:@.iterations
         convert:x:@.arguments/*/max_function_invocations
            type:int

      /*
       * This is the buffer for what iteration we are currently on, used to modify the
       * history request to providing numbering.
       */
      .no-invocation:int:0

      // Buffer prompt for next invocation.
      .new-prompt

      // Contains result and finish reason from OpenAI.
      .result:
      .finish_reason

      // Contains result of function invocation.
      .function-result

      // Invoking OpenAI as longs as we have function invocations and have not invoked it more than a maximum of 5 times.
      while
         mt:x:@.iterations
            .:int:0
         .lambda

            // Resetting buffers.
            set-value:x:@.new-prompt
               .:
            set-value:x:@.result
               .:
            set-value:x:@.finish_reason
               .:

            // True if we've already sent a "waiting for function" message to client.
            .has-sent-waiting:bool:false

            // Invoking OpenAI now with context being either training data or previous messages.
            add:x:./*/http.post/*/payload/*/messages
               get-nodes:x:@.session/*

            // Checking if this is an o3 model.
            if
               eq:x:@.arguments/*/model
                  .:o3-mini
               .lambda

                  // Modifying request.
                  set-name:x:@.lambda/@.lambda/*/http.post/*/payload/*/max_tokens
                     .:max_completion_tokens
                  remove-nodes:x:@.lambda/@.lambda/*/http.post/*/payload/*/temperature

            // Invoking OpenAI's API.
            http.post:"https://api.openai.com/v1/chat/completions"
               convert:bool:true
               headers
                  Authorization:x:@.token
                  Content-Type:application/json
                  Accept:text/event-stream
               payload
                  model:x:@.arguments/*/model
                  max_tokens:x:@.arguments/*/max_tokens
                  temperature:x:@.arguments/*/temperature
                  messages

            // Sanity checking above invocation.
            if
               not
                  and
                     mte:x:@http.post
                        .:int:200
                     lt:x:@http.post
                        .:int:300
               .lambda

                  // Oops, error - Logging error and returning HTTP status code to caller.
                  lambda2hyper:x:@http.post
                  log.error:Something went wrong while invoking OpenAI
                     message:x:@http.post/*/content/*/error/*/message
                     status:x:@http.post
                     error:x:@lambda2hyper

                  // We only throw exception if we're NOT streaming.
                  if
                     or
                        not-exists:x:@.arguments/*/stream
                        eq:x:@.arguments/*/stream
                           .:bool:false
                     .lambda

                        // Not streaming, hence we're on the main thread and can safely throw exceptions.
                        throw:x:@http.post/*/content/*/error/*/message
                           public:bool:true
                           status:x:@http.post

                  else

                     // Signaling client with the error returned from OpenAI.
                     unwrap:x:+/*/*
                     sockets.signal:x:@.arguments/*/session
                        args
                           error:bool:true
                           status:x:@http.post
                           message:x:@http.post/*/content/*/error/*/message

                     // Making sure we stop loop.
                     set-value:x:@.iterations
                        .:int:0

            else

               // Success!
               log.info:Invoking OpenAI was a success

               // Decrementing [.iterations] to avoid invoking OpenAI more than maximum configured amount.
               math.decrement:x:@.iterations

            /*
             * Checking if we're NOT streaming result, at which point we must
             * set above [.result] to response from HTTP invocation.
             */
            if
               or
                  not-exists:x:@.arguments/*/stream
                  eq:x:@.arguments/*/stream
                     .:bool:false
               .lambda

                  /*
                   * Our HTTP POST invocation will return the actual response,
                   * since it's not being streamed using SSE.
                   */
                  set-value:x:@.result
                     get-value:x:@http.post/*/content/*/choices/0/*/message/*/content
                  set-value:x:@.finish_reason
                     get-first-value
                        get-value:x:@http.post/*/content/*/choices/0/*/finish_reason
                        .:unknown

                  // We only iterate if we're streaming response.
                  set-value:x:@.iterations
                     .:int:0

            // Making sure we trim response.
            set-value:x:@.result
               strings.trim:x:@.result

            // Adding current prompt to session, but only if caller provided a session.
            if
               and
                  exists:x:@.arguments/*/session
                  not-null:x:@.arguments/*/session
               .lambda

                  // Caller provided a session.
                  if
                     and
                        not-null:x:@.result
                        neq:x:@.result
                           .:
                     .lambda
                        unwrap:x:+/*/*/*/content
                        add:x:@.session
                           .
                              .
                                 role:assistant
                                 content:x:@.result
                  if
                     and
                        not-null:x:@.new-prompt
                        neq:x:@.new-prompt
                           .:
                     .lambda
                        unwrap:x:+/*/*/*/content
                        add:x:@.session
                           .
                              .
                                 role:system
                                 content:x:@.new-prompt

                  // Removing everything but items that are supposed to be cached.
                  .cache-session
                  add:x:@.cache-session
                     get-nodes:x:@.session/*
                  .tmp
                  set-value:x:@.tmp
                     get-value:x:@.fixed
                  while
                     mt:x:@.tmp
                        .:int:0
                     .lambda

                        // First iteration of invocation
                        remove-nodes:x:@.cache-session/0
                        math.decrement:x:@.tmp

                  // Transforming session to Hyperlambda such that we can store it in cache.
                  lambda2hyper:x:@.cache-session/*

                  // Caching for "session_timeout" seconds (defaults to 20 minutes).
                  cache.set:x:@.arguments/*/session
                     expiration:x:@.arguments/*/session_timeout
                     value:x:@lambda2hyper

            // Checking if type is "supervised" at which point we store request.
            if
               and
                  exists:x:@.arguments/*/supervised
                  not-null:x:@.arguments/*/supervised
                  eq
                     convert:x:@.arguments/*/supervised
                        type:int
                     .:int:1
               .lambda

                  // Opening database connection to store request into ml_requests table.
                  data.connect:[generic|magic]

                     // Creating our prompt field.
                     .prompt
                     if
                        mt:x:@.no-invocation
                           .:int:0
                        .lambda

                           // Adding indexing on history request.
                           set-value:x:@.prompt
                              strings.concat
                                 .:[
                                 get-value:x:@.no-invocation
                                 .:"] - "
                                 get-value:x:@.only-prompt
                     else

                        // Using prompt as was for history request.
                        set-value:x:@.prompt
                           get-value:x:@.only-prompt

                     // Storing response to ml_requests to keep history.
                     .tmp-completion
                     if
                        not-null:x:@.function-result
                        .lambda

                           // We have a function result.
                           set-value:x:@.tmp-completion
                              strings.concat
                                 get-value:x:@.result
                                 .:"\r\n"
                                 .:"\r\n"
                                 .:"Result of function invocation:\r\n"
                                 .:"\r\n"
                                 .:"```json"
                                 .:"\r\n"
                                 get-value:x:@.function-result
                                 .:"\r\n"
                                 .:"```"

                           // Resetting [.function-result] back to null.
                           set-value:x:@.function-result

                     else

                        // No function invocation result.
                        set-value:x:@.tmp-completion
                           get-value:x:@.result

                     data.create
                        table:ml_requests
                        values
                           type:x:@.arguments/*/type
                           prompt:x:@.prompt
                           completion:x:@.tmp-completion
                           finish_reason:x:@.finish_reason
                           session:x:@.arguments/*/session
                           user_id:x:@.arguments/*/user_id
                           referrer:x:@.arguments/*/referrer

            // Checking if we've configured integrations for outgoing messages.
            .outgoing
            set-value:x:@.outgoing
               get-first-value
                  get-value:x:@.arguments/*/webhook_outgoing
                  config.get:"magic:openai:integrations:outgoing:slot"
            if
               and
                  not-null:x:@.outgoing
                  neq:x:@.outgoing
                     .:
               .lambda

                  // Invoking integration slot.
                  .exe

                     // Retrieving URL to invoke.
                     .hook-url
                     set-value:x:@.hook-url
                        get-first-value
                           get-value:x:@.arguments/*/webhook_outgoing_url
                           config.get:"magic:openai:integrations:outgoing:url"

                     // Invoking slot.
                     unwrap:x:./*/signal/*/url
                     signal:x:@.outgoing
                        url:x:@.hook-url

                  // Parametrizing invocation to integration slot.
                  unwrap:x:+/*/*
                  add:x:@.exe/*/signal
                     .
                        result:x:@.result
                        session:x:@.arguments/*/session

                  // Parametrizing invocation to integration slot.
                  if
                     and
                        exists:x:@.arguments/*/to
                        exists:x:@.arguments/*/from
                        not-null:x:@.arguments/*/to
                        not-null:x:@.arguments/*/from
                        strings.contains:x:@.arguments/*/to
                           .:":"
                        strings.contains:x:@.arguments/*/from
                           .:":"
                     .lambda

                        // We have a channel to accommodate for.
                        .channel
                        .to
                        .from
                        strings.split:x:@.arguments/*/to
                           .:":"
                        set-value:x:@.channel
                           get-value:x:@strings.split/0
                        set-value:x:@.to
                           get-value:x:@strings.split/1
                        strings.split:x:@.arguments/*/from
                           .:":"
                        set-value:x:@.from
                           get-value:x:@strings.split/1
                        unwrap:x:+/*/*
                        add:x:@.exe/*/signal
                           .
                              to:x:@.from
                              from:x:@.to
                              channel:x:@.channel

                  else

                     // No channel
                     add:x:@.exe/*/signal
                        get-nodes:x:@.arguments/*/to
                        get-nodes:x:@.arguments/*/from

                  // Invoking callback.
                  eval:x:@.exe

   /*
    * Checking if we're streaming, at which point we invoke above [.invoke]
    * on a different thread.
    */
   if
      get-value:x:@.arguments/*/stream
      .lambda

         // Invoking [.invoke] on a new thread.
         insert-before:x:./*/fork/0
            get-nodes:x:@.invoke
         fork
            eval:x:@.invoke

   else

      // Invoking above [.invoke] on main thread.
      eval:x:@.invoke

   // Checking if we've got references.
   if
      and
         exists:x:@.arguments/*/references
         not-null:x:@.arguments/*/references
         get-value:x:@.arguments/*/references
         exists:x:@signal/*/snippets
      .lambda

         // We've got references, returning these to caller.
         add:x:../*/return
            .
               references
         add:x:../*/return/*/references
            get-nodes:x:@signal/*/snippets/*

   /*
    * Checking if we're NOT streaming, at which point we return the result
    * of the HTTP invocation to caller.
    */
   if
      or
         eq:x:@.arguments/*/stream
            .:bool:false
         not-exists:x:@.arguments/*/stream
      .lambda

         // Not streaming so result was returned by HTTP invocation.
         unwrap:x:+/*/*
         add:x:../*/return
            .
               result:x:@.invoke/*/.result
               finish_reason:x:@.invoke/*/.finish_reason

   // Returning results to caller.
   unwrap:x:./*/return/**
   return
      db_time:x:@signal/*/db_time
      stream:x:@.arguments/*/stream
