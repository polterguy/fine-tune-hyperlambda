
/*
 * This endpoint facilitates communication with an OpenAI-powered chat endpoint using "gpt-xxx" models.
 *
 * INPUT PARAMETERS:
 * - [prompt:string]: The user's message or query for the AI.
 * - [type:string]: An indicator of the conversation type or context.
 * - [recaptcha_response:string]: Google reCAPTCHA response for bot prevention.
 * - [max:int]: A limit on result count or other resource constraints.
 *
 * The endpoint performs the following steps:
 *
 * 1. **Injects logic before callback execution**:
 *    Uses [insert-before] to ensure any pre-callback logic is inserted just before the [.callback] node of a previously registered signal.
 *    This enables modular injection of shared behaviors.
 *
 * 2. **Triggers a shared AI endpoint handler**:
 *    Signals the slot [magic.ai.endpoint-common], which wraps shared logic like authentication, validation, etc.
 *    The [.callback] child contains additional logic specific to this request.
 *
 * 3. **Search operation using arguments**:
 *    Calls [magic.ai.search] to fetch a response, passing all arguments to it.
 *    This slot most likely communicates with OpenAI's GPT models or internal search pipelines.
 *
 * 4. **Extracts and forwards relevant data**:
 *    - Retrieves and returns all [snippets] generated by [magic.ai.search].
 *    - Also returns [db_time], possibly indicating processing time or log data.
 *    - Wraps and returns this data via [return-nodes] under the [snippets] key.
 *
 * 5. **Applies HTTP response caching**:
 *    Sets [Cache-Control:max-age=300], allowing the client (or edge/server) to cache responses for 5 minutes.
 *    This improves performance and reduces redundant OpenAI calls.
 *    More aggressive caching is possible here because the response is deterministic for identical inputs.
 *
 * 6. **Returns the entire response from [magic.ai.endpoint-common]**:
 *    Final call to [return-nodes:x:@signal/*] completes the response cycle and sends all output back to the client.
 *
 * Overall, the endpoint is optimized for consistent, efficient AI-driven responses using modular and reusable slots.
 */

.arguments
   prompt:string
   type:string
   recaptcha_response:string
   max:int

// Invoking slot containing commonalities for all endpoints.
insert-before:x:../*/signal/*/.callback
   get-nodes:x:@.arguments/*
signal:magic.ai.endpoint-common
   .callback

      // Invoking slot responsible for returning URLs to us.
      add:x:+
         get-nodes:x:@.arguments/*
      signal:magic.ai.search

      // Returning results of above invocation to caller.
      add:x:./*/return-nodes/*/snippets
         get-nodes:x:@signal/*/snippets/*
      add:x:./*/return-nodes
         get-nodes:x:@signal/*/db_time
      return-nodes
         snippets

/*
 * Applying some HTTP caching to avoid invoking OpenAI again with
 * the same question before some minimum amount of time has passed.
 *
 * Notice, we can apply more "aggressive" caching here than in our
 * chat endpoints, since the same query will mostly always return
 * the same result, since there is no room for "creativity" here.
 */
response.headers.set
   Cache-Control:max-age=300

// Returning result of worker slot to caller.
return-nodes:x:@signal/*
