
/*
 * Endpoint that asks OpenAI a question using the specified machine learning [type],
 * returning the answer to the caller.
 *
 * This endpoint serves as a legacy alias for the actual chat handler. It forwards the request
 * to the main [/system/openai/chat.get.hl] file, preserving compatibility for existing integrations.
 * All arguments such as [prompt], [type], [references], [chat], [recaptcha_response], [session],
 * and [user_id] are transparently passed through.
 */
.arguments
   prompt:string
   type:string
   references:bool
   chat:bool
   recaptcha_response:string
   session:string
   user_id:string

/*
 * Forward invocation to chat file, since there are no real differences
 * between the two endpoints, since we're checking model type in chat file
 * and acting accordingly.
 *
 * However, we still need to keep this file for legacy reasons.
 */
add:x:+
   get-nodes:x:@.arguments/*
io.file.execute:/system/openai/chat.get.hl
return:x:-/*
