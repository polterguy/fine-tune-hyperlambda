/*
 * Endpoint that asks OpenAI a question using the specified machine learning [type],
 * returning the answer to the caller.
 *
 * This endpoint acts as a **legacy wrapper** around the main AI chat logic located at
 * [/system/openai/chat.get.hl], ensuring backward compatibility for older integrations and clients.
 *
 * INPUT PARAMETERS:
 * - [prompt:string]: The userâ€™s message or query to be answered by the AI model.
 * - [type:string]: Logical model type (e.g., "faq", "support") that determines AI behavior and training data.
 * - [references:bool]: Optional flag to enable retrieval-augmented generation (RAG) using contextual snippets.
 * - [chat:bool]: Indicates if the prompt is part of a multi-turn conversation.
 * - [recaptcha_response:string]: Optional value for Google reCAPTCHA validation.
 * - [session:string]: WebSocket session ID for real-time communication.
 * - [user_id:string]: User identifier for tracking or personalization.
 *
 * EXECUTION LOGIC:
 *
 * 1. **Argument Forwarding**:
 *    - All provided arguments are forwarded as-is to the main handler using [get-nodes:x:@.arguments/*].
 *
 * 2. **Delegation to Chat Handler**:
 *    - Calls [io.file.execute:/system/openai/chat.get.hl], which handles prompt analysis, AI model invocation,
 *      context integration, and any special logic associated with the [type].
 *
 * 3. **Response Relay**:
 *    - Passes the result directly back to the caller using [return:x:-/*].
 *
 * This alias is retained purely for legacy reasons to support integrations that were built before the
 * consolidated chat architecture. Internally, it behaves identically to the main chat endpoint and should
 * eventually be deprecated once all dependencies are migrated.
 */

.arguments
   prompt:string
   type:string
   references:bool
   chat:bool
   recaptcha_response:string
   session:string
   user_id:string

/*
 * Forward invocation to chat file, since there are no real differences
 * between the two endpoints, since we're checking model type in chat file
 * and acting accordingly.
 *
 * However, we still need to keep this file for legacy reasons.
 */
add:x:+
   get-nodes:x:@.arguments/*
io.file.execute:/system/openai/chat.get.hl
return:x:-/*
