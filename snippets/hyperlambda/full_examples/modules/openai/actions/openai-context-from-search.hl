
/*
 * This Hyperlambda endpoint searches DuckDuckGo for the specified [query],
 * extracts contextual snippets from resulting pages, and returns as much
 * relevant content as possible without exceeding the [max_tokens] constraint.
 *
 * 1. [.arguments]
 *    * Accepts:
 *      - [query]: The user's search query (mandatory).
 *      - [max_tokens]: The maximum number of OpenAI-compatible tokens allowed in the returned context (default: 4000).
 *
 * 2. [validators.mandatory], [validators.default]
 *    * Enforces that a [query] is present and applies default value for [max_tokens] if missing.
 *
 * 3. [execute:magic.http.duckduckgo-and-scrape]
 *    * Sends the query to DuckDuckGo and automatically scrapes up to 10 URLs returned in search results.
 *    * The result contains HTML converted to training snippets using the [magic.ai.html.extract] mechanism.
 *
 * 4. [.context] building logic
 *    * Iterates through the result snippets from each URL, appending their [prompt] and [completion] fields.
 *    * Stops adding new snippets once the token length of the concatenated context exceeds [max_tokens].
 *    * Ensures tokens are counted using [openai.tokenize].
 *
 * 5. [remove-nodes]
 *    * Efficiently prunes already-used snippets and URLs to avoid duplication and unnecessary processing.
 *
 * 6. [yield]
 *    * Returns the final trimmed context as a single large Markdown-compatible string.
 *
 * Use Cases:
 * * Constructing rich and relevant AI prompt context from real-time search results.
 * * Bootstrapping fine-tuning datasets with curated snippets for large language models.
 * * Enabling lightweight research agents and auto-context fetchers.
 */
.arguments
   query:string
   max_tokens:int

// Sanity checking invocation.
validators.mandatory:x:@.arguments/*/query

// Applying default values for optional arguments.
validators.default:x:@.arguments
   max_tokens:int:4000

// Retrieving context.
execute:magic.http.duckduckgo-and-scrape
   query:x:@.arguments/*/query
   max:int:10

// Building our context return value.
.context:
while
   and
      exists:x:@execute/0/*/snippets/0
      lt
         openai.tokenize:x:@.context
         convert:x:@.arguments/*/max_tokens
            type:int
   .lambda

      // Temporary value.
      .tmp
      set-value:x:@.tmp
         strings.concat
            get-value:x:@.context
            .:"\n\n"
            get-value:x:@execute/0/*/snippets/0/*/prompt
            .:"\n\n"
            get-value:x:@execute/0/*/snippets/0/*/completion
      if
         lt
            openai.tokenize:x:@.tmp
            convert:x:@.arguments/*/max_tokens
               type:int
         .lambda

            // Still below [max_tokes].
            set-value:x:@.context
               get-value:x:@.tmp

      // Removing top snippet.
      remove-nodes:x:@execute/0/*/snippets/0
      if
         not-exists:x:@execute/0/*/snippets/0
         .lambda

            // TODO: Add reference URL here.

            // Removing currently iterated URL.
            remove-nodes:x:@execute/0

// Trimming result.
set-value:x:@.context
   strings.trim:x:@.context

// Returning result to caller.
yield
   context:x:@.context
