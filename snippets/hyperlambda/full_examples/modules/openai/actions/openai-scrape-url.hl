
/*
 * This snippet creates an HTTP API endpoint that scrapes a given [url], extracts its HTML content, and converts it to Markdown.
 *
 * 1. [.arguments]
 *    * Accepts a single required parameter:
 *      - [url]: The URL to be scraped. Must be a valid and complete HTTP or HTTPS URL.
 *
 * 2. [validators.mandatory] + [validators.url]
 *    * Ensures that the [url] argument is provided and is syntactically valid.
 *
 * 3. [if] - Scheme Normalization
 *    * Checks if the given [url] includes a scheme (e.g., "http://" or "https://").
 *    * If not, prepends "https://" to the [url] to default to a secure scheme.
 *
 * 4. [http.get]
 *    * Performs a GET request on the validated [url].
 *    * Sets headers:
 *      - [User-Agent] to identify the scraper.
 *      - [Accept] to specify HTML is the expected content.
 *      - [Accept-Encoding] to avoid gzip/deflate compression complications.
 *    * Timeout is set to 30 seconds.
 *
 * 5. Sanity Checks on HTTP Status
 *    * Validates that the response status is between 200 and 299.
 *    * If not, logs the error and returns the raw HTTP result to the caller.
 *
 * 6. Sanity Checks on Content
 *    * Verifies that the response includes HTML content.
 *    * Fails with a 415 (Unsupported Media Type) status if the returned data is missing, null, empty, or has a [Content-Type] other than [text/html].
 *
 * 7. [html2markdown]
 *    * Converts the fetched HTML content into Markdown format.
 *    * Passes the original URL as context to improve relative link handling in the Markdown output.
 *
 * 8. [yield]
 *    * Returns:
 *      - [result]: The converted Markdown content.
 *      - [urls]: List of URLs extracted during the conversion (e.g., hyperlinks, image sources).
 *
 * **Use cases:**
 * * Importing blog posts or documentation from websites for training or storage.
 * * Creating offline-readable Markdown content from webpages.
 * * Crawling specific HTML sources to extract readable, clean documentation.
 */
.arguments
   url:string

// Sanity checking invocation.
validators.mandatory:x:@.arguments/*/url

// Prepending scheme if not given.
if
   and
      not
         strings.starts-with:x:@.arguments/*/url
            .:"http://"
      not
         strings.starts-with:x:@.arguments/*/url
            .:"https://"
   .lambda
   
      // Defaulting to https
      set-value:x:@.arguments/*/url
         strings.concat
            .:"https://"
            get-value:x:@.arguments/*/url

// Sanity checking invocation.
validators.url:x:@.arguments/*/url

// Retrieving HTML from URL.
http.get:x:@.arguments/*/url
   timeout:int:30
   headers
      User-Agent:AINIRO-Crawler 2.0
      Accept:text/html
      Accept-Encoding:identity

// Sanity checking result from above invocation.
if
   not
      and
         mte:x:@http.get
            .:int:200
         lt:x:@http.get
            .:int:300
   .lambda

      // Oops, logging error.
      lambda2hyper:x:@http.get
      log.error:Something went wrong while trying to scrape URL
         url:x:@.arguments/*/url
         error:x:@lambda2hyper
      return:x:@http.get

// Making sure URL returned HTML.
if
   or
      not-exists:x:@http.get/*/content
      null:x:@http.get/*/content
      eq:x:@http.get/*/content
         .:
      not-exists:x:@http.get/*/headers/*/Content-Type
      not
         strings.starts-with:x:@http.get/*/headers/*/Content-Type
            .:text/html
   .lambda

      // Oops, doing some basic logging.
      lambda2hyper:x:@http.get
      log.error:URL did not return HTML content when scraping
         content:x:@lambda2hyper
         url:x:@.arguments/*/url
      return:int:415

// Transforming result to HTML
html2markdown:x:@http.get/*/content
   url:x:@.arguments/*/url

// Returning result of above tranformation to caller.
yield
   result:x:@html2markdown
   urls:x:@html2markdown/*/urls/*
